#!/usr/bin/env python3
"""
è¯„ä¼°ä¼˜åŒ–åçš„æ¨¡å‹æ€§èƒ½ - ä½¿ç”¨ä¸¥æ ¼æ¨¡å¼
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from test_cases_strict import ALL_STRICT_TEST_CASES, CRITICAL_TEST_CASES
from evaluate_model import ModelEvaluator
import json

def main():
    print("=" * 70)
    print("ğŸ“Š è¯„ä¼°ä¼˜åŒ–åçš„æ¨¡å‹ - ä¸¥æ ¼æ¨¡å¼")
    print("=" * 70)

    # ä½¿ç”¨æœ€æ–°çš„checkpoint
    model_path = "outputs/checkpoint-873"
    base_model = "models/Qwen/Qwen2.5-3B"

    print(f"\nğŸ”µ æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"ğŸ“¦ åŸºåº§æ¨¡å‹: {base_model}")

    # åˆå§‹åŒ–è¯„ä¼°å™¨
    print("\nğŸ”µ åŠ è½½æ¨¡å‹...")
    evaluator = ModelEvaluator(
        model_path=model_path,
        base_model=base_model
    )

    try:
        evaluator.load_model()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # 1. ä½¿ç”¨ä¸¥æ ¼æ¨¡å¼è¯„ä¼°æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
    print("\n" + "=" * 70)
    print("ğŸ”µ ä¸¥æ ¼æ¨¡å¼è¯„ä¼° - æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹")
    print("=" * 70)

    result_all = evaluator.evaluate_accuracy(ALL_STRICT_TEST_CASES, strict_mode=True)

    # 2. ä»…è¯„ä¼°å…³é”®æµ‹è¯•ç”¨ä¾‹
    print("\n" + "=" * 70)
    print("ğŸ”µ ä¸¥æ ¼æ¨¡å¼è¯„ä¼° - å…³é”®æµ‹è¯•ç”¨ä¾‹")
    print("=" * 70)

    result_critical = evaluator.evaluate_accuracy(CRITICAL_TEST_CASES, strict_mode=True)

    # 3. ä¿å­˜ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ")
    print("=" * 70)

    results = {
        "all_tests": result_all,
        "critical_tests": result_critical,
        "model_info": {
            "model_path": model_path,
            "base_model": base_model,
            "training_data": "æŠ¥é”€ç»†åˆ™_combined.jsonl (863+298æ¡)",
            "optimization": "å¼ºè°ƒæ•°å­—ç±»é—®é¢˜"
        }
    }

    output_file = f"evaluation_optimized_{Path(model_path).name}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    # 4. æ‰“å°æ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ“Š è¯„ä¼°æ€»ç»“")
    print("=" * 70)

    print(f"\næ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ ({len(ALL_STRICT_TEST_CASES)}ä¸ª):")
    print(f"  ç²¾ç¡®åº¦: {result_all['accuracy']*100:.1f}%")
    print(f"  æ­£ç¡®æ•°: {result_all['correct_count']}/{len(ALL_STRICT_TEST_CASES)}")
    print(f"  ç»¼åˆè¯„åˆ†: {result_all.get('overall_score', 0):.0f}/100")

    print(f"\nå…³é”®æµ‹è¯•ç”¨ä¾‹ ({len(CRITICAL_TEST_CASES)}ä¸ª):")
    print(f"  ç²¾ç¡®åº¦: {result_critical['accuracy']*100:.1f}%")
    print(f"  æ­£ç¡®æ•°: {result_critical['correct_count']}/{len(CRITICAL_TEST_CASES)}")
    print(f"  ç»¼åˆè¯„åˆ†: {result_critical.get('overall_score', 0):.0f}/100")

    # è®¡ç®—æ•°å­—ç±»é—®é¢˜å‡†ç¡®ç‡
    all_number_tests = [t for t in ALL_STRICT_TEST_CASES if any(k.strip().isdigit() for k in t['keywords'])]
    if all_number_tests:
        correct_number = sum(1 for r in result_all['detailed_results']
                            if r['is_correct'] and any(k.strip().isdigit() for k in r['expected_keywords']))
        print(f"\næ•°å­—ç±»é—®é¢˜ ({len(all_number_tests)}ä¸ª):")
        print(f"  å‡†ç¡®ç‡: {correct_number/len(all_number_tests)*100:.1f}%")
        print(f"  æ­£ç¡®æ•°: {correct_number}/{len(all_number_tests)}")

    print("\n" + "=" * 70)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("=" * 70)

    return results

if __name__ == "__main__":
    main()
