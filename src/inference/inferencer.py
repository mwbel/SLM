"""æ¨¡å‹æ¨ç†æ¨¡å—"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from pathlib import Path
import platform


class ModelInferencer:
    """æ¨¡å‹æ¨ç†å™¨ - ç”¨äºåŠ è½½å’Œæ¨ç†è®­ç»ƒå¥½çš„æ¨¡å‹"""

    def __init__(self, model_path: str, base_model: str = "models/Qwen/Qwen2.5-1.5B"):
        """
        åˆå§‹åŒ–æ¨ç†å™¨

        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ï¼ˆLoRAæƒé‡ï¼‰
            base_model: åŸºåº§æ¨¡å‹åç§°
        """
        self.model_path = Path(model_path)
        self.base_model = base_model
        self.model = None
        self.tokenizer = None
        self.device = self._get_device()

    def _get_device(self):
        """
        è‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡
        ä¼˜å…ˆçº§: CUDA GPU > Apple MPS > CPU

        Returns:
            str: è®¾å¤‡ç±»å‹ ('cuda', 'mps', æˆ– 'cpu')
        """
        # ä¼˜å…ˆæ£€æµ‹CUDA GPU
        if torch.cuda.is_available():
            return "cuda"
        # å…¶æ¬¡æ£€æµ‹Apple MPS (macOS GPUåŠ é€Ÿ)
        elif torch.backends.mps.is_available() and platform.system() == "Darwin":
            return "mps"
        # æœ€åå›é€€åˆ°CPU
        else:
            return "cpu"

    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹...")

        # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
        if self.device == "cuda":
            gpu_name = torch.cuda.get_device_name(0)
            print(f"ğŸ–¥ï¸  è®¾å¤‡: CUDA GPU ({gpu_name})")
        elif self.device == "mps":
            print(f"ğŸ–¥ï¸  è®¾å¤‡: Apple MPS (Metal Performance Shaders)")
        else:
            print(f"ğŸ–¥ï¸  è®¾å¤‡: CPU")

        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not self.model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")

        # ç¦ç”¨tqdmè¿›åº¦æ¡ä»¥é¿å…åœ¨Gradioä¸Šä¸‹æ–‡ä¸­å‡ºç°BrokenPipeError
        import os
        disable_tqdm = os.environ.get('DISABLE_TQDM', '1')  # é»˜è®¤ç¦ç”¨

        from transformers import utils
        utils.logging.set_verbosity_error()  # ç¦ç”¨è¯¦ç»†æ—¥å¿—
        os.environ['TRANSFORMERS_NO_PROGRESS_BAR'] = '1'  # ç¦ç”¨è¿›åº¦æ¡

        # æ™ºèƒ½æ£€æµ‹åŸºåº§æ¨¡å‹è·¯å¾„
        # 1. é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
        local_path = Path(self.base_model)
        if local_path.exists():
            print(f"ä½¿ç”¨æœ¬åœ°åŸºåº§æ¨¡å‹: {local_path.absolute()}")
            base_model_path = str(local_path.absolute())
        # 2. æ£€æŸ¥ModelScopeç¼“å­˜
        elif (Path.home() / ".cache/modelscope" / self.base_model).exists():
            modelscope_path = Path.home() / ".cache/modelscope" / self.base_model
            print(f"ä»ModelScopeç¼“å­˜åŠ è½½åŸºåº§æ¨¡å‹: {modelscope_path}")
            base_model_path = str(modelscope_path)
        else:
            print(f"âš ï¸  æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°è¯•ä»HuggingFaceä¸‹è½½: {self.base_model}")
            base_model_path = self.base_model

        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=False,
            local_files_only=True  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token

        # åŠ è½½åŸºåº§æ¨¡å‹
        print("åŠ è½½åŸºåº§æ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float32 if self.device == "cpu" else torch.float16,
            device_map="auto" if self.device != "cpu" else None,
            trust_remote_code=False,
            local_files_only=True,  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )

        # åŠ è½½LoRAæƒé‡
        print(f"åŠ è½½LoRAæƒé‡: {self.model_path}")
        self.model = PeftModel.from_pretrained(base_model, str(self.model_path))

        # åˆå¹¶æƒé‡ä»¥æé«˜æ¨ç†é€Ÿåº¦
        print("åˆå¹¶LoRAæƒé‡...")
        self.model = self.model.merge_and_unload()

        # ç§»åŠ¨åˆ°è®¾å¤‡
        if self.device == "cpu":
            self.model = self.model.to("cpu")

        self.model.eval()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def generate(
        self,
        prompt: str,
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 50,
        repetition_penalty: float = 1.2,
        add_prompt_format: bool = True
    ) -> str:
        """
        ç”Ÿæˆå›å¤

        Args:
            prompt: è¾“å…¥æç¤º
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: nucleus samplingå‚æ•°
            top_k: top-k samplingå‚æ•°
            repetition_penalty: é‡å¤æƒ©ç½šç³»æ•°
            add_prompt_format: æ˜¯å¦è‡ªåŠ¨æ·»åŠ "é—®é¢˜ï¼šå›ç­”ï¼š"æ ¼å¼ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if self.model is None or self.tokenizer is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")

        # æ ¼å¼åŒ–è¾“å…¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if add_prompt_format:
            formatted_prompt = f"é—®é¢˜ï¼š{prompt}\nå›ç­”ï¼š"
        else:
            formatted_prompt = prompt

        # Tokenize
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )

        # ç§»åŠ¨åˆ°è®¾å¤‡
        if self.device != "cpu":
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                repetition_penalty=repetition_penalty,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                no_repeat_ngram_size=3,  # é˜²æ­¢3-gramé‡å¤
            )

        # è·å–è¾“å…¥é•¿åº¦
        input_length = inputs["input_ids"].shape[1]

        # è§£ç  (åªè§£ç æ–°ç”Ÿæˆçš„token)
        new_tokens = outputs[0][input_length:]
        generated_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)

        # é˜²æ­¢æ¨¡å‹ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜
        if "é—®é¢˜ï¼š" in generated_text:
            generated_text = generated_text.split("é—®é¢˜ï¼š")[0]

        return generated_text.strip()

    def chat(self, message: str, history: list = None) -> tuple:
        """
        å¯¹è¯æ¥å£ï¼ˆå…¼å®¹Gradioï¼‰

        æ³¨æ„ï¼šå¯¹äºè´¢åŠ¡æŠ¥é”€è§„åˆ™ç­‰ç‹¬ç«‹é—®é¢˜ï¼Œä¸ä½¿ç”¨å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡
        è¿™æ ·å¯ä»¥é¿å…ä¸Šä¸‹æ–‡æˆªæ–­é—®é¢˜ï¼Œå¹¶ä¿æŒä¸è¯„ä¼°è„šæœ¬ä¸€è‡´çš„è¡Œä¸º

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            history: å¯¹è¯å†å²ï¼ˆä¿ç•™ç”¨äºUIæ˜¾ç¤ºï¼Œä½†ä¸ç”¨äºæ¨ç†ï¼‰

        Returns:
            æ›´æ–°åçš„å†å²
        """
        if history is None:
            history = []

        # ç›´æ¥ä½¿ç”¨å½“å‰é—®é¢˜ç”Ÿæˆå›å¤ï¼ˆä¸ä½¿ç”¨å†å²ä¸Šä¸‹æ–‡ï¼‰
        # ä½¿ç”¨ä¸è¯„ä¼°è„šæœ¬ç›¸åŒçš„å‚æ•°
        response = self.generate(
            message,
            max_new_tokens=300,  # ä¸è¯„ä¼°è„šæœ¬ä¸€è‡´
            temperature=0.1,     # ä¸è¯„ä¼°è„šæœ¬ä¸€è‡´
            top_p=0.95,          # ä¸è¯„ä¼°è„šæœ¬ä¸€è‡´
            repetition_penalty=1.0,  # ä¸è¯„ä¼°è„šæœ¬ä¸€è‡´
            add_prompt_format=True  # ä½¿ç”¨"é—®é¢˜ï¼šå›ç­”ï¼š"æ ¼å¼
        )

        # æ›´æ–°å†å²
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": response})

        return history

    def unload_model(self):
        """å¸è½½æ¨¡å‹é‡Šæ”¾å†…å­˜"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None

        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        print("æ¨¡å‹å·²å¸è½½")
