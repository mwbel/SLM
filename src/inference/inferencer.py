"""æ¨¡å‹æ¨ç†æ¨¡å—"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from pathlib import Path
import platform


class ModelInferencer:
    """æ¨¡å‹æ¨ç†å™¨ - ç”¨äºåŠ è½½å’Œæ¨ç†è®­ç»ƒå¥½çš„æ¨¡å‹"""

    def __init__(self, model_path: str, base_model: str = "models/Qwen/Qwen2.5-1.5B"):
        """
        åˆå§‹åŒ–æ¨ç†å™¨

        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ï¼ˆLoRAæƒé‡ï¼‰
            base_model: åŸºåº§æ¨¡å‹åç§°
        """
        self.model_path = Path(model_path)
        self.base_model = base_model
        self.model = None
        self.tokenizer = None
        self.device = self._get_device()

    def _get_device(self):
        """
        è‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡
        ä¼˜å…ˆçº§: CUDA GPU > Apple MPS > CPU

        Returns:
            str: è®¾å¤‡ç±»å‹ ('cuda', 'mps', æˆ– 'cpu')
        """
        # ä¼˜å…ˆæ£€æµ‹CUDA GPU
        if torch.cuda.is_available():
            return "cuda"
        # å…¶æ¬¡æ£€æµ‹Apple MPS (macOS GPUåŠ é€Ÿ)
        elif torch.backends.mps.is_available() and platform.system() == "Darwin":
            return "mps"
        # æœ€åå›é€€åˆ°CPU
        else:
            return "cpu"

    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹...")

        # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
        if self.device == "cuda":
            gpu_name = torch.cuda.get_device_name(0)
            print(f"ğŸ–¥ï¸  è®¾å¤‡: CUDA GPU ({gpu_name})")
        elif self.device == "mps":
            print(f"ğŸ–¥ï¸  è®¾å¤‡: Apple MPS (Metal Performance Shaders)")
        else:
            print(f"ğŸ–¥ï¸  è®¾å¤‡: CPU")

        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not self.model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")

        # æ™ºèƒ½æ£€æµ‹åŸºåº§æ¨¡å‹è·¯å¾„
        # 1. é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
        local_path = Path(self.base_model)
        if local_path.exists():
            print(f"ä½¿ç”¨æœ¬åœ°åŸºåº§æ¨¡å‹: {local_path.absolute()}")
            base_model_path = str(local_path.absolute())
        # 2. æ£€æŸ¥ModelScopeç¼“å­˜
        elif (Path.home() / ".cache/modelscope" / self.base_model).exists():
            modelscope_path = Path.home() / ".cache/modelscope" / self.base_model
            print(f"ä»ModelScopeç¼“å­˜åŠ è½½åŸºåº§æ¨¡å‹: {modelscope_path}")
            base_model_path = str(modelscope_path)
        else:
            print(f"âš ï¸  æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°è¯•ä»HuggingFaceä¸‹è½½: {self.base_model}")
            base_model_path = self.base_model

        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=False,
            local_files_only=True  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token

        # åŠ è½½åŸºåº§æ¨¡å‹
        print("åŠ è½½åŸºåº§æ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float32 if self.device == "cpu" else torch.float16,
            device_map="auto" if self.device != "cpu" else None,
            trust_remote_code=False,
            local_files_only=True,  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )

        # åŠ è½½LoRAæƒé‡
        print(f"åŠ è½½LoRAæƒé‡: {self.model_path}")
        self.model = PeftModel.from_pretrained(base_model, str(self.model_path))

        # åˆå¹¶æƒé‡ä»¥æé«˜æ¨ç†é€Ÿåº¦
        print("åˆå¹¶LoRAæƒé‡...")
        self.model = self.model.merge_and_unload()

        # ç§»åŠ¨åˆ°è®¾å¤‡
        if self.device == "cpu":
            self.model = self.model.to("cpu")

        self.model.eval()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def generate(
        self,
        prompt: str,
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 50,
        repetition_penalty: float = 1.2,
        add_prompt_format: bool = True
    ) -> str:
        """
        ç”Ÿæˆå›å¤

        Args:
            prompt: è¾“å…¥æç¤º
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: nucleus samplingå‚æ•°
            top_k: top-k samplingå‚æ•°
            repetition_penalty: é‡å¤æƒ©ç½šç³»æ•°
            add_prompt_format: æ˜¯å¦è‡ªåŠ¨æ·»åŠ "é—®é¢˜ï¼šå›ç­”ï¼š"æ ¼å¼ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if self.model is None or self.tokenizer is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")

        # æ ¼å¼åŒ–è¾“å…¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if add_prompt_format:
            formatted_prompt = f"é—®é¢˜ï¼š{prompt}\nå›ç­”ï¼š"
        else:
            formatted_prompt = prompt

        # Tokenize
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )

        # ç§»åŠ¨åˆ°è®¾å¤‡
        if self.device != "cpu":
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                repetition_penalty=repetition_penalty,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                no_repeat_ngram_size=3,  # é˜²æ­¢3-gramé‡å¤
            )

        # è·å–è¾“å…¥é•¿åº¦
        input_length = inputs["input_ids"].shape[1]

        # è§£ç  (åªè§£ç æ–°ç”Ÿæˆçš„token)
        new_tokens = outputs[0][input_length:]
        generated_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)

        # é˜²æ­¢æ¨¡å‹ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜
        if "é—®é¢˜ï¼š" in generated_text:
            generated_text = generated_text.split("é—®é¢˜ï¼š")[0]

        return generated_text.strip()

    def chat(self, message: str, history: list = None) -> tuple:
        """
        å¯¹è¯æ¥å£ï¼ˆå…¼å®¹Gradioï¼‰- æ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            history: å¯¹è¯å†å²

        Returns:
            (å›å¤, æ›´æ–°åçš„å†å²)
        """
        if history is None:
            history = []

        # æ„å»ºåŒ…å«å†å²çš„ä¸Šä¸‹æ–‡æç¤º
        context_prompt = ""

        # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€å¤šä¿ç•™æœ€è¿‘3è½®å¯¹è¯ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿ï¼‰
        recent_history = history[-6:] if len(history) > 6 else history
        for msg in recent_history:
            if msg["role"] == "user":
                context_prompt += f"é—®é¢˜ï¼š{msg['content']}\n"
            elif msg["role"] == "assistant":
                context_prompt += f"å›ç­”ï¼š{msg['content']}\n"

        # æ·»åŠ å½“å‰é—®é¢˜
        context_prompt += f"é—®é¢˜ï¼š{message}\nå›ç­”ï¼š"

        # ç”Ÿæˆå›å¤ï¼ˆä½¿ç”¨åŒ…å«ä¸Šä¸‹æ–‡çš„æç¤ºï¼Œä¸å†æ·»åŠ æ ¼å¼ï¼‰
        response = self.generate(context_prompt, add_prompt_format=False)

        # æ›´æ–°å†å²
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": response})

        return history

    def unload_model(self):
        """å¸è½½æ¨¡å‹é‡Šæ”¾å†…å­˜"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None

        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        print("æ¨¡å‹å·²å¸è½½")
