"""
WorkflowManager - æµç¨‹æ§åˆ¶å™¨

è´Ÿè´£ï¼š
1. ä¸²è”å¤šä¸ª Skill å½¢æˆå®Œæ•´çš„å¤„ç†æµç¨‹
2. æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼ˆä¿å­˜å’Œæ¢å¤å¤„ç†è¿›åº¦ï¼‰
3. é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
4. ç”Ÿæˆå¤„ç†æŠ¥å‘Š
"""

import asyncio
import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

from .base_skill import BaseSkill
from .router_file import FileRouterSkill
from .parser_native import NativeParserSkill
from .parser_pdf_ocr import OCRParserSkill
from .chunk_smart import SmartChunkerSkill


class WorkflowManager:
    """
    å·¥ä½œæµç®¡ç†å™¨

    ä¸²è”å¤šä¸ª Skillï¼Œå®ç°å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹
    """

    def __init__(self,
                 checkpoint_dir: str = '.workflow_checkpoints',
                 enable_checkpoint: bool = True,
                 max_retries: int = 3):
        """
        åˆå§‹åŒ– WorkflowManager

        Args:
            checkpoint_dir: æ–­ç‚¹æ–‡ä»¶ä¿å­˜ç›®å½•
            enable_checkpoint: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ 
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.enable_checkpoint = enable_checkpoint
        self.max_retries = max_retries

        # åˆå§‹åŒ– Skills
        self.router = FileRouterSkill()
        self.native_parser = NativeParserSkill()
        self.ocr_parser = OCRParserSkill(checkpoint_dir=str(self.checkpoint_dir))
        self.chunker = SmartChunkerSkill()

        # å·¥ä½œæµçŠ¶æ€
        self.workflow_id = None
        self.current_step = None
        self.results = {}

    async def process_file(self,
                          file_path: str,
                          chunk_size: int = 1000,
                          overlap: int = 200,
                          chunking_strategy: str = 'smart',
                          resume: bool = True) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶çš„å®Œæ•´æµç¨‹

        æµç¨‹ï¼š
        1. æ–‡ä»¶è·¯ç”±ï¼ˆè¯†åˆ«ç±»å‹ï¼‰
        2. æ–‡æ¡£è§£æï¼ˆåŸç”Ÿæˆ– OCRï¼‰
        3. æ™ºèƒ½åˆ‡åˆ†
        4. è¿”å›ç»“æœ

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            chunk_size: åˆ‡åˆ†å—å¤§å°
            overlap: åˆ‡åˆ†é‡å å¤§å°
            chunking_strategy: åˆ‡åˆ†ç­–ç•¥
            resume: æ˜¯å¦ä»æ–­ç‚¹æ¢å¤

        Returns:
            {
                'success': bool,
                'file_path': str,
                'chunks': List[Dict],
                'metadata': dict,
                'workflow_report': dict
            }
        """
        file_path = Path(file_path)
        self.workflow_id = f"{file_path.stem}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path.name}")
        print(f"   å·¥ä½œæµ ID: {self.workflow_id}")
        print('='*60)

        # æ£€æŸ¥æ–­ç‚¹
        checkpoint_file = self.checkpoint_dir / f"{file_path.stem}_workflow.json"
        if resume and self.enable_checkpoint and checkpoint_file.exists():
            print(f"ğŸ“‚ å‘ç°æ–­ç‚¹æ–‡ä»¶ï¼Œå°è¯•æ¢å¤...")
            checkpoint = self._load_checkpoint(checkpoint_file)
            return await self._resume_workflow(checkpoint, file_path, chunk_size, overlap, chunking_strategy)

        # å¼€å§‹æ–°çš„å·¥ä½œæµ
        workflow_start = datetime.now()

        try:
            # Step 1: æ–‡ä»¶è·¯ç”±
            print(f"\nğŸ“ Step 1: æ–‡ä»¶è·¯ç”±")
            self.current_step = 'routing'
            self._save_workflow_checkpoint(checkpoint_file, {
                'workflow_id': self.workflow_id,
                'file_path': str(file_path),
                'current_step': 'routing',
                'timestamp': datetime.now().isoformat()
            })

            route_result = await self.router.run(str(file_path))
            if not route_result['success']:
                raise Exception(f"æ–‡ä»¶è·¯ç”±å¤±è´¥: {route_result['error']}")

            route_data = route_result['data']
            print(f"   âœ… æ–‡ä»¶ç±»å‹: {route_data['file_type']}")
            print(f"   âœ… æ¨èè§£æå™¨: {route_data['recommended_parser']}")

            # Step 2: æ–‡æ¡£è§£æ
            print(f"\nğŸ“„ Step 2: æ–‡æ¡£è§£æ")
            self.current_step = 'parsing'
            self._save_workflow_checkpoint(checkpoint_file, {
                'workflow_id': self.workflow_id,
                'file_path': str(file_path),
                'current_step': 'parsing',
                'route_data': route_data,
                'timestamp': datetime.now().isoformat()
            })

            if route_data['recommended_parser'] == 'ocr':
                print(f"   ä½¿ç”¨ OCR è§£æå™¨")
                parse_result = await self.ocr_parser.run(route_data, resume=resume)
            else:
                print(f"   ä½¿ç”¨åŸç”Ÿè§£æå™¨")
                parse_result = await self.native_parser.run(route_data)

            if not parse_result['success']:
                raise Exception(f"æ–‡æ¡£è§£æå¤±è´¥: {parse_result['error']}")

            parse_data = parse_result['data']
            print(f"   âœ… è§£æå®Œæˆ: {parse_data['metadata']['char_count']} å­—ç¬¦")

            # Step 3: æ™ºèƒ½åˆ‡åˆ†
            print(f"\nâœ‚ï¸  Step 3: æ™ºèƒ½åˆ‡åˆ†")
            self.current_step = 'chunking'
            self._save_workflow_checkpoint(checkpoint_file, {
                'workflow_id': self.workflow_id,
                'file_path': str(file_path),
                'current_step': 'chunking',
                'route_data': route_data,
                'parse_data': parse_data,
                'timestamp': datetime.now().isoformat()
            })

            chunk_result = await self.chunker.run(
                parse_data,
                chunk_size=chunk_size,
                overlap=overlap
            )

            if not chunk_result['success']:
                raise Exception(f"æ–‡æœ¬åˆ‡åˆ†å¤±è´¥: {chunk_result['error']}")

            chunk_data = chunk_result['data']
            print(f"   âœ… åˆ‡åˆ†å®Œæˆ: {chunk_data['chunk_count']} ä¸ªå—")

            # è®¡ç®—æ€»è€—æ—¶
            workflow_end = datetime.now()
            total_time = (workflow_end - workflow_start).total_seconds()

            # ç”Ÿæˆå·¥ä½œæµæŠ¥å‘Š
            workflow_report = {
                'workflow_id': self.workflow_id,
                'file_path': str(file_path),
                'total_time': round(total_time, 2),
                'steps': {
                    'routing': {
                        'success': True,
                        'time': route_result['execution_time'],
                        'result': route_data
                    },
                    'parsing': {
                        'success': True,
                        'time': parse_result['execution_time'],
                        'parser': route_data['recommended_parser']
                    },
                    'chunking': {
                        'success': True,
                        'time': chunk_result['execution_time'],
                        'chunk_count': chunk_data['chunk_count']
                    }
                }
            }

            # æ¸…ç†æ–­ç‚¹æ–‡ä»¶
            if checkpoint_file.exists():
                checkpoint_file.unlink()
                print(f"\nğŸ—‘ï¸  å·²æ¸…ç†æ–­ç‚¹æ–‡ä»¶")

            print(f"\n{'='*60}")
            print(f"âœ… å·¥ä½œæµå®Œæˆ!")
            print(f"   æ€»è€—æ—¶: {total_time:.2f} ç§’")
            print(f"   ç”Ÿæˆå—æ•°: {chunk_data['chunk_count']}")
            print('='*60)

            return {
                'success': True,
                'file_path': str(file_path),
                'chunks': chunk_data['chunks'],
                'metadata': {
                    'file_type': route_data['file_type'],
                    'is_scanned': route_data.get('is_scanned', False),
                    'original_length': parse_data['metadata']['char_count'],
                    'chunk_count': chunk_data['chunk_count'],
                    'chunk_size': chunk_size,
                    'overlap': overlap,
                    'strategy': chunking_strategy
                },
                'workflow_report': workflow_report
            }

        except Exception as e:
            print(f"\nâŒ å·¥ä½œæµå¤±è´¥: {e}")

            # ä¿å­˜é”™è¯¯çŠ¶æ€
            self._save_workflow_checkpoint(checkpoint_file, {
                'workflow_id': self.workflow_id,
                'file_path': str(file_path),
                'current_step': self.current_step,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            })

            return {
                'success': False,
                'file_path': str(file_path),
                'error': str(e),
                'current_step': self.current_step
            }

    async def _resume_workflow(self,
                               checkpoint: Dict,
                               file_path: Path,
                               chunk_size: int,
                               overlap: int,
                               chunking_strategy: str) -> Dict[str, Any]:
        """
        ä»æ–­ç‚¹æ¢å¤å·¥ä½œæµ

        Args:
            checkpoint: æ–­ç‚¹æ•°æ®
            file_path: æ–‡ä»¶è·¯å¾„
            chunk_size: åˆ‡åˆ†å—å¤§å°
            overlap: åˆ‡åˆ†é‡å å¤§å°
            chunking_strategy: åˆ‡åˆ†ç­–ç•¥

        Returns:
            å¤„ç†ç»“æœ
        """
        print(f"ğŸ”„ ä»æ–­ç‚¹æ¢å¤å·¥ä½œæµ")
        print(f"   ä¸Šæ¬¡æ­¥éª¤: {checkpoint.get('current_step')}")
        print(f"   æ—¶é—´: {checkpoint.get('timestamp')}")

        current_step = checkpoint.get('current_step')

        # æ ¹æ®æ–­ç‚¹æ­¥éª¤å†³å®šä»å“ªé‡Œç»§ç»­
        if current_step == 'routing':
            # ä»å¤´å¼€å§‹
            print(f"   ä»æ–‡ä»¶è·¯ç”±æ­¥éª¤é‡æ–°å¼€å§‹")
            return await self.process_file(file_path, chunk_size, overlap, chunking_strategy, resume=False)

        elif current_step == 'parsing':
            # ä»è§£ææ­¥éª¤ç»§ç»­
            print(f"   ä»æ–‡æ¡£è§£ææ­¥éª¤ç»§ç»­")
            route_data = checkpoint.get('route_data')

            if route_data['recommended_parser'] == 'ocr':
                # OCR è§£æå™¨è‡ªå¸¦æ–­ç‚¹ç»­ä¼ 
                parse_result = await self.ocr_parser.run(route_data, resume=True)
            else:
                parse_result = await self.native_parser.run(route_data)

            if not parse_result['success']:
                raise Exception(f"æ–‡æ¡£è§£æå¤±è´¥: {parse_result['error']}")

            parse_data = parse_result['data']

            # ç»§ç»­åˆ‡åˆ†
            chunk_result = await self.chunker.run(parse_data, chunk_size=chunk_size, overlap=overlap)

            if not chunk_result['success']:
                raise Exception(f"æ–‡æœ¬åˆ‡åˆ†å¤±è´¥: {chunk_result['error']}")

            chunk_data = chunk_result['data']

            return {
                'success': True,
                'file_path': str(file_path),
                'chunks': chunk_data['chunks'],
                'metadata': {
                    'file_type': route_data['file_type'],
                    'chunk_count': chunk_data['chunk_count']
                }
            }

        elif current_step == 'chunking':
            # ä»åˆ‡åˆ†æ­¥éª¤ç»§ç»­
            print(f"   ä»æ–‡æœ¬åˆ‡åˆ†æ­¥éª¤ç»§ç»­")
            parse_data = checkpoint.get('parse_data')

            chunk_result = await self.chunker.run(parse_data, chunk_size=chunk_size, overlap=overlap)

            if not chunk_result['success']:
                raise Exception(f"æ–‡æœ¬åˆ‡åˆ†å¤±è´¥: {chunk_result['error']}")

            chunk_data = chunk_result['data']

            return {
                'success': True,
                'file_path': str(file_path),
                'chunks': chunk_data['chunks'],
                'metadata': {
                    'chunk_count': chunk_data['chunk_count']
                }
            }

        else:
            print(f"   æœªçŸ¥çš„æ–­ç‚¹æ­¥éª¤ï¼Œä»å¤´å¼€å§‹")
            return await self.process_file(file_path, chunk_size, overlap, chunking_strategy, resume=False)

    async def process_directory(self,
                                input_dir: str,
                                output_dir: str,
                                chunk_size: int = 1000,
                                overlap: int = 200,
                                chunking_strategy: str = 'smart') -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶

        Args:
            input_dir: è¾“å…¥ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            chunk_size: åˆ‡åˆ†å—å¤§å°
            overlap: åˆ‡åˆ†é‡å å¤§å°
            chunking_strategy: åˆ‡åˆ†ç­–ç•¥

        Returns:
            æ‰¹é‡å¤„ç†ç»“æœ
        """
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        print(f"\n{'='*60}")
        print(f"ğŸ“ æ‰¹é‡å¤„ç†ç›®å½•: {input_dir}")
        print('='*60)

        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        supported_extensions = ['.txt', '.md', '.docx', '.pdf', '.png', '.jpg', '.jpeg']

        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶
        files = [
            f for f in input_path.iterdir()
            if f.is_file() and f.suffix.lower() in supported_extensions
        ]

        print(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")

        results = []
        success_count = 0
        error_count = 0

        for i, file_path in enumerate(files, 1):
            print(f"\n{'='*60}")
            print(f"å¤„ç†æ–‡ä»¶ {i}/{len(files)}: {file_path.name}")
            print('='*60)

            try:
                result = await self.process_file(
                    str(file_path),
                    chunk_size=chunk_size,
                    overlap=overlap,
                    chunking_strategy=chunking_strategy
                )

                if result['success']:
                    # ä¿å­˜ç»“æœ
                    output_file = output_path / f"{file_path.stem}_chunks.json"
                    self._save_chunks(output_file, result['chunks'])
                    print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")
                    success_count += 1
                else:
                    print(f"âŒ å¤„ç†å¤±è´¥: {result.get('error')}")
                    error_count += 1

                results.append(result)

            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {e}")
                error_count += 1
                results.append({
                    'success': False,
                    'file_path': str(file_path),
                    'error': str(e)
                })

        print(f"\n{'='*60}")
        print(f"ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ")
        print(f"   æ€»æ–‡ä»¶æ•°: {len(files)}")
        print(f"   æˆåŠŸ: {success_count}")
        print(f"   å¤±è´¥: {error_count}")
        print('='*60)

        return {
            'total_files': len(files),
            'success_count': success_count,
            'error_count': error_count,
            'results': results
        }

    def _save_workflow_checkpoint(self, checkpoint_file: Path, data: dict):
        """ä¿å­˜å·¥ä½œæµæ–­ç‚¹"""
        if not self.enable_checkpoint:
            return

        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _load_checkpoint(self, checkpoint_file: Path) -> dict:
        """åŠ è½½å·¥ä½œæµæ–­ç‚¹"""
        with open(checkpoint_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _save_chunks(self, output_file: Path, chunks: List[Dict]):
        """ä¿å­˜åˆ‡åˆ†ç»“æœ"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import asyncio

    async def test_workflow():
        """æµ‹è¯•å·¥ä½œæµç®¡ç†å™¨"""
        manager = WorkflowManager(
            checkpoint_dir='.workflow_checkpoints',
            enable_checkpoint=True
        )

        # æµ‹è¯•å•ä¸ªæ–‡ä»¶
        print("\n=== æµ‹è¯•å•ä¸ªæ–‡ä»¶å¤„ç† ===")
        result = await manager.process_file(
            file_path="data/example.pdf",
            chunk_size=1000,
            overlap=200,
            chunking_strategy='smart',
            resume=True
        )

        if result['success']:
            print(f"\nâœ… å¤„ç†æˆåŠŸ:")
            print(f"   æ–‡ä»¶: {result['file_path']}")
            print(f"   å—æ•°: {result['metadata']['chunk_count']}")
            print(f"   æ€»è€—æ—¶: {result['workflow_report']['total_time']} ç§’")
        else:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {result['error']}")

        # æµ‹è¯•æ‰¹é‡å¤„ç†
        print("\n\n=== æµ‹è¯•æ‰¹é‡å¤„ç† ===")
        batch_result = await manager.process_directory(
            input_dir="data/documents",
            output_dir="data/output",
            chunk_size=1000,
            overlap=200
        )

        print(f"\næ‰¹é‡å¤„ç†ç»“æœ:")
        print(f"   æˆåŠŸ: {batch_result['success_count']}")
        print(f"   å¤±è´¥: {batch_result['error_count']}")

    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_workflow())
