"""è®­ç»ƒå™¨"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import Trainer as HFTrainer, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
import platform
import os
from pathlib import Path


class Trainer:
    """QLoRAè®­ç»ƒå™¨"""

    def __init__(self, model_name: str, config: dict):
        self.model_name = model_name
        self.config = config

        # å°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹
        # 1. æ£€æŸ¥æ˜¯å¦æ˜¯ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚ models/Qwen/...ï¼‰
        local_path = Path(model_name)
        if local_path.exists():
            print(f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {local_path.absolute()}")
            model_path = str(local_path.absolute())
        # 2. æ£€æŸ¥ModelScopeç¼“å­˜
        elif (Path.home() / ".cache/modelscope" / model_name).exists():
            modelscope_path = Path.home() / ".cache/modelscope" / model_name
            print(f"âœ… ä»ModelScopeç¼“å­˜åŠ è½½æ¨¡å‹: {modelscope_path}")
            model_path = str(modelscope_path)
        else:
            print(f"âš ï¸  æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°è¯•ä»HuggingFaceä¸‹è½½")
            model_path = model_name

        # è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜å…ˆä½¿ç”¨ModelScope
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ï¼ˆä¼˜å…ˆçº§ï¼šCUDA > MPS > CPUï¼‰
        device_info = self._detect_device()
        use_quantization = device_info["use_quantization"]

        print(f"ğŸ–¥ï¸  è®¾å¤‡æ£€æµ‹: {device_info['device_name']}")
        if device_info['device_type'] == 'cuda':
            print(f"   âœ… ä½¿ç”¨CUDA GPUåŠ é€Ÿ (æ”¯æŒ4-bité‡åŒ–)")
        elif device_info['device_type'] == 'mps':
            print(f"   âœ… ä½¿ç”¨Apple MPSåŠ é€Ÿ (Metal Performance Shaders)")
        else:
            print(f"   âš ï¸  ä½¿ç”¨CPUæ¨¡å¼ (è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢)")

        if use_quantization:
            # 4-bité‡åŒ–é…ç½®ï¼ˆä»…CUDA GPUï¼‰
            from transformers import BitsAndBytesConfig

            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
            )

            # åŠ è½½æ¨¡å‹å’Œtokenizer
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=False,
                local_files_only=True,  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            )
        else:
            # éé‡åŒ–æ¨¡å¼ï¼ˆMPSæˆ–CPUï¼‰
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float32,
                device_map=device_info["device_map"],
                trust_remote_code=False,
                local_files_only=True,  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            )

        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=False,
            local_files_only=True  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token

        # å‡†å¤‡æ¨¡å‹ç”¨äºè®­ç»ƒ
        if use_quantization:
            self.model = prepare_model_for_kbit_training(self.model)

        # LoRAé…ç½®
        lora_config = LoraConfig(
            r=config.get("lora", {}).get("rank", 8),
            lora_alpha=config.get("lora", {}).get("alpha", 16),
            lora_dropout=config.get("lora", {}).get("dropout", 0.05),
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            task_type="CAUSAL_LM",
        )
        self.model = get_peft_model(self.model, lora_config)

        # æ‰“å°å¯è®­ç»ƒå‚æ•°
        self.model.print_trainable_parameters()

    def _detect_device(self):
        """
        è‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡
        ä¼˜å…ˆçº§: CUDA GPU > Apple MPS > CPU

        Returns:
            dict: åŒ…å«è®¾å¤‡ä¿¡æ¯çš„å­—å…¸
                - device_type: 'cuda', 'mps', æˆ– 'cpu'
                - device_name: è®¾å¤‡æè¿°åç§°
                - device_map: ç”¨äºæ¨¡å‹åŠ è½½çš„device_mapå‚æ•°
                - use_quantization: æ˜¯å¦ä½¿ç”¨4-bité‡åŒ–
        """
        # ä¼˜å…ˆæ£€æµ‹CUDA GPU
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            return {
                "device_type": "cuda",
                "device_name": f"CUDA GPU ({gpu_name})",
                "device_map": "auto",
                "use_quantization": True
            }

        # å…¶æ¬¡æ£€æµ‹Apple MPS (macOS GPUåŠ é€Ÿ)
        if torch.backends.mps.is_available() and platform.system() == "Darwin":
            return {
                "device_type": "mps",
                "device_name": "Apple MPS (Metal Performance Shaders)",
                "device_map": "auto",
                "use_quantization": False
            }

        # æœ€åå›é€€åˆ°CPU
        return {
            "device_type": "cpu",
            "device_name": "CPU",
            "device_map": "cpu",
            "use_quantization": False
        }

    def train(self, train_data: list):
        """æ‰§è¡Œè®­ç»ƒ"""
        # å‡†å¤‡æ•°æ®é›†
        dataset = Dataset.from_list(train_data)

        def tokenize_function(examples):
            # æ ¼å¼åŒ–ä¸ºå¯¹è¯æ ¼å¼
            instruction = examples.get("instruction", "")
            output = examples.get("output", "")
            text = f"é—®é¢˜ï¼š{instruction}\nå›ç­”ï¼š{output}"

            # å¯¹æ–‡æœ¬è¿›è¡Œtokenization
            tokenized = self.tokenizer(
                text,
                truncation=True,
                max_length=self.config.get("model", {}).get("max_seq_length", 512),
                padding="max_length",
                return_tensors=None,
            )

            # åˆ›å»ºlabelsï¼šåªè®­ç»ƒ"å›ç­”"éƒ¨åˆ†ï¼Œå°†"é—®é¢˜"éƒ¨åˆ†è®¾ä¸º-100
            labels = tokenized["input_ids"].copy()

            # æ‰¾åˆ°"å›ç­”ï¼š"çš„ä½ç½®
            prompt_length = len(self.tokenizer(f"é—®é¢˜ï¼š{instruction}\nå›ç­”ï¼š", add_special_tokens=False)["input_ids"])

            # å°†é—®é¢˜éƒ¨åˆ†çš„labelsè®¾ä¸º-100ï¼ˆPyTorchä¼šå¿½ç•¥è¿™äº›ä½ç½®çš„lossï¼‰
            labels[:prompt_length] = [-100] * prompt_length

            tokenized["labels"] = labels

            return tokenized

        tokenized_dataset = dataset.map(tokenize_function, batched=False)

        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.config.get("paths", {}).get("output_dir", "./outputs"),
            num_train_epochs=self.config.get("training", {}).get("num_epochs", 3),
            per_device_train_batch_size=self.config.get("training", {}).get(
                "batch_size", 1
            ),
            gradient_accumulation_steps=self.config.get("training", {}).get(
                "gradient_accumulation_steps", 4
            ),
            learning_rate=float(
                self.config.get("training", {}).get("learning_rate", 2e-4)
            ),
            warmup_steps=self.config.get("training", {}).get("warmup_steps", 10),
            logging_steps=1,
            save_strategy="epoch",
            save_total_limit=2,
            fp16=False,  # macOSä¸æ”¯æŒfp16
            report_to="none",
        )

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = HFTrainer(
            model=self.model, args=training_args, train_dataset=tokenized_dataset
        )

        # æ‰§è¡Œè®­ç»ƒ
        trainer.train()

    def save_model(self, output_path: str):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(output_path, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.model.save_pretrained(output_path)
        self.tokenizer.save_pretrained(output_path)
