"""Gradio UIåº”ç”¨"""

import gradio as gr
import os
import sys
from pathlib import Path

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# å®šä½ slm-trainer æ ¹ç›®å½•
CURRENT_DIR = Path(__file__).resolve().parent
SLM_TRAINER_DIR = CURRENT_DIR.parent.parent
DATA_DIR = SLM_TRAINER_DIR / "data"
OUTPUTS_DIR = SLM_TRAINER_DIR / "outputs"
DOMAIN_KNOWLEDGE_DIR = SLM_TRAINER_DIR.parent / "domain_knowledge"


def list_files(directory, extension=None):
    """åˆ—å‡ºæŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶"""
    files = []
    dir_path = Path(directory)
    if not dir_path.exists():
        return files

    for f in dir_path.iterdir():
        if f.is_file() and (extension is None or f.name.endswith(extension)):
            files.append(str(f))
    return sorted(files)


def list_directories(directory):
    """åˆ—å‡ºæŒ‡å®šç›®å½•ä¸‹çš„å­ç›®å½•"""
    dirs = []
    dir_path = Path(directory)
    if not dir_path.exists():
        return dirs

    for f in dir_path.iterdir():
        if f.is_dir() and not f.name.startswith("."):
            dirs.append(str(f))
    return sorted(dirs)


BASE_MODELS = [
    "models/Qwen/Qwen2.5-1.5B",  # å‡çº§åˆ°1.5Bï¼ˆæ¨èï¼‰
    "models/Qwen/Qwen2.5-3B",    # æ›´å¤§æ¨¡å‹ï¼ˆå¦‚éœ€æ›´é«˜ç²¾åº¦ï¼‰
    "models/Qwen/Qwen2.5-0.5B",  # è½»é‡çº§ï¼ˆå¦‚éœ€æ›´å¿«é€Ÿåº¦ï¼‰
    "Qwen/Qwen2.5-1.5B",         # HuggingFaceå¤‡ç”¨
    "Qwen/Qwen2.5-3B",
    "Qwen/Qwen2.5-0.5B",
]


def list_all_pdfs(root_dir):
    """é€’å½’åˆ—å‡ºæ‰€æœ‰PDFæ–‡ä»¶"""
    pdfs = []
    root = Path(root_dir)
    if not root.exists():
        return pdfs
    for path in root.rglob("*.pdf"):
        pdfs.append(str(path))
    return sorted(pdfs)


def find_citation(response, pdf_path):
    """åœ¨PDFä¸­æŸ¥æ‰¾å¼•ç”¨é¡µç """
    if not pdf_path or not os.path.exists(pdf_path):
        return None

    try:
        import fitz

        doc = fitz.open(pdf_path)
        best_page = -1
        max_score = 0

        # ç®€å•ç®—æ³•ï¼šå­—ç¬¦é‡å ç‡ (Jaccard Similarity)
        response_chars = set(response)
        if not response_chars:
            return None

        for i, page in enumerate(doc):
            text = page.get_text()
            page_chars = set(text)

            # é¿å…é™¤ä»¥é›¶
            if not len(response_chars):
                continue

            common = response_chars & page_chars
            score = len(common) / len(response_chars)

            # é˜ˆå€¼ 0.15
            if score > max_score and score > 0.15:
                max_score = score
                best_page = i + 1

        doc.close()

        if best_page != -1:
            return best_page
        return None
    except Exception as e:
        print(f"Citation search failed: {e}")
        return None


def create_ui():
    """åˆ›å»ºGradioç•Œé¢"""

    def process_file(file, num_pairs, progress=gr.Progress()):
        """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶"""
        if file is None:
            return "è¯·å…ˆä¸Šä¼ æ–‡ä»¶", ""

        try:
            progress(0, desc="å¼€å§‹å¤„ç†...")

            # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¯åŠ¨æ—¶åŠ è½½
            from data_prep import DataDistiller

            progress(0.2, desc="åˆå§‹åŒ–æ•°æ®è’¸é¦å™¨...")
            # åˆå§‹åŒ–æ•°æ®è’¸é¦å™¨
            distiller = DataDistiller()

            # è·å–ä¸Šä¼ æ–‡ä»¶è·¯å¾„
            file_path = file.name

            progress(0.4, desc="åˆ†ææ–‡æ¡£ç»“æ„...")
            from data_prep.distiller import extract_text

            text = extract_text(file_path)
            total_len = len(text)

            # æ™ºèƒ½åˆ†å—ç­–ç•¥ï¼šæ ¹æ®ç›®æ ‡ç”Ÿæˆæ•°é‡åŠ¨æ€è°ƒæ•´åˆ†å—å¤§å°
            # ç›®æ ‡ï¼šè®©æ¨¡å‹åœ¨æ¯ä¸ªå°å—ä¸Šç”Ÿæˆçº¦ 10-15 æ¡æ•°æ®ï¼Œé¿å…"å·æ‡’"
            target_pairs = int(num_pairs)
            pairs_per_chunk = 15  # è¿™æ˜¯ä¸€ä¸ªæ¨¡å‹æ¯”è¾ƒèˆ’é€‚ç”Ÿæˆçš„æ•°é‡

            # è®¡ç®—éœ€è¦çš„å—æ•°
            target_chunks = max(1, int(target_pairs / pairs_per_chunk))

            # è®¡ç®—åˆ†å—å¤§å°
            chunk_size = int(total_len / target_chunks)

            # è¾¹ç•Œé™åˆ¶ (é™åˆ¶åœ¨ 1000-6000 å­—ç¬¦ä¹‹é—´ï¼Œä¿è¯ä¸Šä¸‹æ–‡å®Œæ•´æ€§)
            chunk_size = max(1000, min(chunk_size, 6000))

            progress(0.5, desc=f"æ‰§è¡Œåˆ†å—å¤„ç† (æ¯å—çº¦ {chunk_size} å­—ç¬¦)...")

            # ä½¿ç”¨åˆ†å—å¤„ç†
            output_path = distiller.process_file_chunked(
                file_path=file_path,
                output_dir=str(DATA_DIR),
                num_pairs_per_chunk=pairs_per_chunk,
                chunk_size=chunk_size,
                overlap=200,
            )

            progress(0.8, desc="ç”Ÿæˆè®­ç»ƒæ•°æ®...")
            # è¯»å–ç”Ÿæˆçš„JSONLå†…å®¹é¢„è§ˆ
            preview = ""
            with open(output_path, "r", encoding="utf-8") as f:
                lines = f.readlines()[:3]  # åªæ˜¾ç¤ºå‰3æ¡
                preview = "\n".join(lines)

            progress(1.0, desc="å®Œæˆï¼")
            status = f"âœ… å¤„ç†å®Œæˆï¼\n\næ–‡ä»¶: {Path(file_path).name}\nè¾“å‡º: {output_path}\nç”Ÿæˆå¯¹è¯å¯¹æ•°é‡: {len(lines)}"

            return status, preview

        except Exception as e:
            import traceback

            error_detail = traceback.format_exc()
            return f"âŒ å¤„ç†å¤±è´¥: {str(e)}\n\nè¯¦ç»†é”™è¯¯:\n{error_detail}", ""

    def get_api_status():
        """è·å–APIå¯†é’¥çŠ¶æ€"""
        try:
            from data_prep import DataDistiller

            distiller = DataDistiller()
            return distiller.get_status_report()
        except Exception as e:
            return f"æ— æ³•è·å–çŠ¶æ€: {str(e)}"

    def start_training(
        data_file, model_name, epochs, batch_size, learning_rate, progress=gr.Progress()
    ):
        """å¼€å§‹è®­ç»ƒ"""
        trainer = None
        try:
            import yaml
            import json
            import torch
            from training import Trainer
            from utils import setup_logger

            progress(0, desc="æ£€æŸ¥è®­ç»ƒæ•°æ®...")
            # æ£€æŸ¥æ•°æ®æ–‡ä»¶
            if not data_file or not os.path.exists(data_file):
                return "âŒ è¯·å…ˆé€‰æ‹©æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®æ–‡ä»¶"

            progress(0.1, desc="åŠ è½½é…ç½®...")
            # åŠ è½½é…ç½®
            config_path = Path(__file__).parent.parent.parent / "config.yaml"
            with open(config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)

            # æ›´æ–°é…ç½®
            config["model"]["base_model"] = model_name
            config["training"]["num_epochs"] = int(epochs)
            config["training"]["batch_size"] = int(batch_size)
            config["training"]["learning_rate"] = float(learning_rate)

            progress(0.2, desc="åŠ è½½è®­ç»ƒæ•°æ®...")
            # åŠ è½½è®­ç»ƒæ•°æ®
            train_data = []
            with open(data_file, "r", encoding="utf-8") as f:
                for line in f:
                    train_data.append(json.loads(line.strip()))

            status = f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(train_data)} æ¡æ ·æœ¬\n"
            status += f"ğŸ¤– åŸºåº§æ¨¡å‹: {model_name}\n"
            status += f"ğŸ“ˆ è®­ç»ƒè½®æ•°: {epochs}\n"
            status += f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}\n"
            status += f"ğŸ¯ å­¦ä¹ ç‡: {learning_rate}\n\n"

            progress(0.3, desc="åˆå§‹åŒ–è®­ç»ƒå™¨(åŠ è½½æ¨¡å‹)...")
            status += "ğŸš€ æ­£åœ¨åˆå§‹åŒ–è®­ç»ƒå™¨...\n"

            # åˆå§‹åŒ–è®­ç»ƒå™¨
            trainer = Trainer(model_name=model_name, config=config)

            progress(0.5, desc="è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
            status += "âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ\n\n"
            status += "ğŸƒ å¼€å§‹è®­ç»ƒ...\n"

            progress(0.6, desc="æ‰§è¡Œè®­ç»ƒ...")
            # å¼€å§‹è®­ç»ƒ
            trainer.train(train_data)

            progress(0.9, desc="è®­ç»ƒå®Œæˆ,ä¿å­˜æ¨¡å‹...")
            status += "âœ… è®­ç»ƒå®Œæˆï¼\n\n"

            # ä¿å­˜æ¨¡å‹
            output_path = str(OUTPUTS_DIR / "trained_model")
            trainer.save_model(output_path)

            progress(1.0, desc="å®Œæˆï¼")
            status += f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}\n"

            return status

        except Exception as e:
            import traceback

            error_detail = traceback.format_exc()
            return f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}\n\nè¯¦ç»†é”™è¯¯:\n{error_detail}"
        finally:
            # æ¸…ç†èµ„æº
            if trainer:
                try:
                    # æ¸…ç†GPUå†…å­˜
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                except Exception:
                    pass  # å¿½ç•¥æ¸…ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯

    with gr.Blocks(title="SLM Trainer") as app:
        gr.Markdown("# SLM Trainer - å‚ç›´å°æ¨¡å‹è®­ç»ƒå·¥å…·")

        with gr.Tab("æ•°æ®å‡†å¤‡"):
            gr.Markdown("## ğŸ“„ æ–‡æ¡£ä¸Šä¼ ä¸çŸ¥è¯†è’¸é¦")
            gr.Markdown("ä¸Šä¼ PDFæˆ–TXTæ–‡ä»¶ï¼Œä½¿ç”¨Gemini APIè‡ªåŠ¨æå–çŸ¥è¯†å¹¶ç”Ÿæˆè®­ç»ƒæ•°æ®")

            with gr.Row():
                with gr.Column():
                    file_input = gr.File(
                        label="ä¸Šä¼ æ–‡ä»¶ (PDF/TXT)", file_types=[".pdf", ".txt"]
                    )
                    num_pairs_input = gr.Number(
                        label="ç”Ÿæˆæ ·æœ¬æ•°é‡",
                        value=100,
                        minimum=10,
                        maximum=1000,
                        step=10,
                    )
                    process_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")

                with gr.Column():
                    status_output = gr.Textbox(
                        label="å¤„ç†çŠ¶æ€", lines=6, interactive=False
                    )

            gr.Markdown("### ç”Ÿæˆæ•°æ®é¢„è§ˆ")
            preview_output = gr.Textbox(
                label="JSONLé¢„è§ˆ (å‰3æ¡)", lines=10, interactive=False
            )

            gr.Markdown("### APIå¯†é’¥çŠ¶æ€")
            api_status_btn = gr.Button("ğŸ” æŸ¥çœ‹å¯†é’¥çŠ¶æ€")
            api_status_output = gr.Textbox(
                label="å¯†é’¥ä½¿ç”¨æƒ…å†µ", lines=15, interactive=False
            )

            # ç»‘å®šäº‹ä»¶
            process_btn.click(
                fn=process_file,
                inputs=[file_input, num_pairs_input],
                outputs=[status_output, preview_output],
            )

            api_status_btn.click(
                fn=get_api_status, inputs=[], outputs=[api_status_output]
            )

        with gr.Tab("æ¨¡å‹è®­ç»ƒ"):
            gr.Markdown("## ğŸ¯ æ¨¡å‹è®­ç»ƒ")
            gr.Markdown("é…ç½®è®­ç»ƒå‚æ•°å¹¶å¼€å§‹å¾®è°ƒæ¨¡å‹")

            with gr.Row():
                with gr.Column():
                    with gr.Row():
                        data_file_input = gr.Dropdown(
                            label="è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„",
                            choices=list_files(DATA_DIR, ".jsonl"),
                            value=(
                                list_files(DATA_DIR, ".jsonl")[0]
                                if list_files(DATA_DIR, ".jsonl")
                                else None
                            ),
                            allow_custom_value=True,
                            interactive=True,
                            scale=3,
                        )
                        refresh_data_btn = gr.Button("ğŸ”„", scale=0, min_width=50)

                    model_name_input = gr.Dropdown(
                        label="åŸºåº§æ¨¡å‹",
                        choices=BASE_MODELS,
                        value="models/Qwen/Qwen2.5-1.5B",  # æ›´æ–°ä¸º1.5B
                        allow_custom_value=True,
                        interactive=True,
                    )

                with gr.Column():
                    epochs_input = gr.Number(
                        label="è®­ç»ƒè½®æ•°", value=3, minimum=1, maximum=10
                    )
                    batch_size_input = gr.Number(
                        label="æ‰¹æ¬¡å¤§å°", value=2, minimum=1, maximum=8
                    )
                    learning_rate_input = gr.Number(
                        label="å­¦ä¹ ç‡", value=0.0002, minimum=0.00001, maximum=0.001
                    )

            train_btn = gr.Button("ğŸš€ å¼€å§‹è®­ç»ƒ", variant="primary", size="lg")
            progress = gr.Textbox(label="è®­ç»ƒè¿›åº¦", lines=15, interactive=False)

            # ç»‘å®šåˆ·æ–°æŒ‰é’®
            def refresh_data_files():
                files = list_files(DATA_DIR, ".jsonl")
                return gr.Dropdown(choices=files, value=files[0] if files else None)

            refresh_data_btn.click(fn=refresh_data_files, outputs=[data_file_input])

            # ç»‘å®šè®­ç»ƒæŒ‰é’®
            train_btn.click(
                fn=start_training,
                inputs=[
                    data_file_input,
                    model_name_input,
                    epochs_input,
                    batch_size_input,
                    learning_rate_input,
                ],
                outputs=[progress],
            )

        with gr.Tab("æ¨¡å‹æµ‹è¯•"):
            gr.Markdown("## ğŸ’¬ æ¨¡å‹å¯¹è¯æµ‹è¯•")
            gr.Markdown("åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå¯¹è¯æµ‹è¯•")

            with gr.Row():
                with gr.Column(scale=1):
                    with gr.Row():
                        model_path_input = gr.Dropdown(
                            label="æ¨¡å‹è·¯å¾„",
                            choices=list_directories(OUTPUTS_DIR),
                            value=str(OUTPUTS_DIR / "trained_model"),
                            allow_custom_value=True,
                            interactive=True,
                            scale=3,
                        )
                        refresh_model_btn = gr.Button("ğŸ”„", scale=0, min_width=50)

                    base_model_input = gr.Dropdown(
                        label="åŸºåº§æ¨¡å‹",
                        choices=BASE_MODELS,
                        value="models/Qwen/Qwen2.5-1.5B",  # æ›´æ–°ä¸º1.5B
                        allow_custom_value=True,
                        interactive=True,
                    )

                    ref_doc_input = gr.Dropdown(
                        label="å‚è€ƒæ–‡æ¡£ (ç”¨äºç”Ÿæˆå¼•ç”¨é“¾æ¥)",
                        choices=list_all_pdfs(DOMAIN_KNOWLEDGE_DIR),
                        value=None,
                        allow_custom_value=True,
                        interactive=True,
                    )
                    load_model_btn = gr.Button("ğŸš€ åŠ è½½æ¨¡å‹", variant="primary")
                    model_status = gr.Textbox(
                        label="æ¨¡å‹çŠ¶æ€", value="æœªåŠ è½½", interactive=False, lines=3
                    )
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯")

                with gr.Column(scale=2):
                    chatbot = gr.Chatbot(type="messages", height=500)
                    msg = gr.Textbox(
                        label="è¾“å…¥æ¶ˆæ¯", placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜...", lines=2
                    )
                    with gr.Row():
                        send_btn = gr.Button("å‘é€", variant="primary")
                        stop_btn = gr.Button("åœæ­¢")

            # ç»‘å®šäº‹ä»¶
            load_model_btn.click(
                fn=load_model_for_chat,
                inputs=[model_path_input, base_model_input],
                outputs=[model_status],
            )

            msg.submit(
                fn=chat_with_model,
                inputs=[msg, chatbot, ref_doc_input],
                outputs=[chatbot, msg],
            )

            send_btn.click(
                fn=chat_with_model,
                inputs=[msg, chatbot, ref_doc_input],
                outputs=[chatbot, msg],
            )

            clear_btn.click(fn=lambda: [], inputs=[], outputs=[chatbot])

            # ç»‘å®šåˆ·æ–°æŒ‰é’®
            def refresh_model_dirs():
                dirs = list_directories(OUTPUTS_DIR)
                default_model = str(OUTPUTS_DIR / "reimbursement_model")
                return gr.Dropdown(
                    choices=dirs,
                    value=(
                        default_model
                        if default_model in dirs
                        else (dirs[0] if dirs else None)
                    ),
                )

            refresh_model_btn.click(fn=refresh_model_dirs, outputs=[model_path_input])

    return app


# å…¨å±€å˜é‡å­˜å‚¨æ¨ç†å™¨
_inferencer = None


def load_model_for_chat(model_path: str, base_model: str):
    """åŠ è½½æ¨¡å‹ç”¨äºå¯¹è¯"""
    global _inferencer
    try:
        from inference import ModelInferencer

        # å¸è½½æ—§æ¨¡å‹
        if _inferencer is not None:
            _inferencer.unload_model()

        # åŠ è½½æ–°æ¨¡å‹
        _inferencer = ModelInferencer(model_path, base_model)
        _inferencer.load_model()

        return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\nè·¯å¾„: {model_path}\nè®¾å¤‡: {_inferencer.device}"

    except Exception as e:
        import traceback

        error_detail = traceback.format_exc()
        return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}\n\nè¯¦ç»†é”™è¯¯:\n{error_detail}"


def chat_with_model(message: str, history: list, ref_doc: str = None):
    """ä¸æ¨¡å‹å¯¹è¯"""
    global _inferencer

    if not message or not message.strip():
        return history, ""

    if _inferencer is None:
        # å¦‚æœæ¨¡å‹æœªåŠ è½½ï¼Œè¿”å›æç¤º
        history.append({"role": "user", "content": message})
        history.append(
            {
                "role": "assistant",
                "content": "âš ï¸ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆç‚¹å‡»'åŠ è½½æ¨¡å‹'æŒ‰é’®åŠ è½½æ¨¡å‹ã€‚",
            }
        )
        return history, ""

    try:
        # è°ƒç”¨æ¨ç†å™¨ç”Ÿæˆå›å¤
        updated_history = _inferencer.chat(message, history)

        # å¦‚æœæŒ‡å®šäº†å‚è€ƒæ–‡æ¡£ï¼Œå°è¯•æŸ¥æ‰¾å¼•ç”¨
        if ref_doc and os.path.exists(ref_doc):
            # è·å–æœ€æ–°å›å¤
            last_response = updated_history[-1]["content"]

            # æŸ¥æ‰¾é¡µç 
            page = find_citation(last_response, ref_doc)

            if page:
                doc_name = Path(ref_doc).name
                # æ·»åŠ å¼•ç”¨é“¾æ¥ (Gradioæ”¯æŒ file=è·¯å¾„ çš„é“¾æ¥)
                # æ ¼å¼: [æ˜¾ç¤ºæ–‡æœ¬](/file=ç»å¯¹è·¯å¾„#page=é¡µç )
                citation = f"\n\n> ğŸ“š **å‚è€ƒæ¥æº**: [{doc_name} ç¬¬ {page} é¡µ](/file={ref_doc}#page={page})"
                updated_history[-1]["content"] += citation

        return updated_history, ""

    except Exception as e:
        import traceback

        error_detail = traceback.format_exc()
        history.append({"role": "user", "content": message})
        history.append(
            {
                "role": "assistant",
                "content": f"âŒ ç”Ÿæˆå›å¤å¤±è´¥: {str(e)}\n\n{error_detail}",
            }
        )
        return history, ""
