#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–æ¨¡å‹è¯„ä¼°è„šæœ¬
æä¾›å¤šç»´åº¦ã€å¤šæŒ‡æ ‡çš„æ¨¡å‹æ•ˆæœè¯„ä¼°
"""

import sys
import json
from pathlib import Path
from datetime import datetime
import random

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from inference import ModelInferencer


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""

    def __init__(self, model_path: str, base_model: str):
        self.model_path = model_path
        self.base_model = base_model
        self.inferencer = None
        self.results = []

    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
        self.inferencer = ModelInferencer(self.model_path, self.base_model)
        self.inferencer.load_model()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")

    def evaluate_accuracy(
        self,
        test_cases: list,
        temperature: float = 0.1
    ) -> dict:
        """
        ç²¾ç¡®åº¦è¯„ä¼° - åŸºäºå…³é”®è¯åŒ¹é…

        Args:
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ [{"question": "", "keywords": [], "description": ""}]
            temperature: ç”Ÿæˆæ¸©åº¦

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print("=" * 70)
        print("ğŸ“Š ç²¾ç¡®åº¦è¯„ä¼°")
        print("=" * 70)

        correct_count = 0
        total_count = len(test_cases)
        detailed_results = []

        for i, case in enumerate(test_cases, 1):
            question = case["question"]
            keywords = case["keywords"]
            description = case.get("description", "")

            print(f"\n[{i}/{total_count}] {description}")
            print(f"é—®é¢˜: {question}")
            print(f"é¢„æœŸå…³é”®è¯: {', '.join(keywords)}")

            # ç”Ÿæˆå›ç­”
            response = self.inferencer.generate(
                question,
                max_new_tokens=300,
                temperature=temperature,
                top_p=0.95,
                repetition_penalty=1.0,
            )

            print(f"æ¨¡å‹å›ç­”: {response}")

            # å…³é”®è¯åŒ¹é…
            found_keywords = [kw for kw in keywords if kw in response]
            missing_keywords = [kw for kw in keywords if kw not in response]

            # è®¡ç®—å¾—åˆ†
            keyword_coverage = len(found_keywords) / len(keywords) if keywords else 0

            print(f"\nå…³é”®è¯åŒ¹é…:")
            print(f"  âœ“ æ‰¾åˆ° ({len(found_keywords)}/{len(keywords)}): {', '.join(found_keywords)}")
            if missing_keywords:
                print(f"  âœ— ç¼ºå°‘ ({len(missing_keywords)}): {', '.join(missing_keywords)}")

            # åˆ¤æ–­æ˜¯å¦åˆæ ¼ï¼ˆè‡³å°‘åŒ…å«50%å…³é”®è¯ï¼‰
            is_correct = keyword_coverage >= 0.5
            if is_correct:
                correct_count += 1
                print(f"  âœ… åˆæ ¼ (è¦†ç›–ç‡: {keyword_coverage*100:.1f}%)")
            else:
                print(f"  âŒ ä¸åˆæ ¼ (è¦†ç›–ç‡: {keyword_coverage*100:.1f}%)")

            # ä¿å­˜è¯¦ç»†ç»“æœ
            detailed_results.append({
                "question": question,
                "description": description,
                "expected_keywords": keywords,
                "found_keywords": found_keywords,
                "missing_keywords": missing_keywords,
                "keyword_coverage": keyword_coverage,
                "response": response,
                "is_correct": is_correct
            })

            print("-" * 70)

        # æ€»ç»“
        accuracy = correct_count / total_count if total_count > 0 else 0
        avg_coverage = sum(r["keyword_coverage"] for r in detailed_results) / total_count

        print("\n" + "=" * 70)
        print("ğŸ“ˆ ç²¾ç¡®åº¦è¯„ä¼°æ€»ç»“")
        print("=" * 70)
        print(f"æ€»æµ‹è¯•æ•°: {total_count}")
        print(f"åˆæ ¼æ•°: {correct_count}")
        print(f"åˆæ ¼ç‡: {accuracy*100:.1f}%")
        print(f"å¹³å‡å…³é”®è¯è¦†ç›–ç‡: {avg_coverage*100:.1f}%")
        print("=" * 70 + "\n")

        return {
            "type": "accuracy",
            "total": total_count,
            "correct": correct_count,
            "accuracy": accuracy,
            "avg_coverage": avg_coverage,
            "detailed_results": detailed_results
        }

    def evaluate_diversity(
        self,
        test_questions: list,
        num_samples: int = 3
    ) -> dict:
        """
        å¤šæ ·æ€§è¯„ä¼° - æµ‹è¯•æ¨¡å‹å¯¹åŒä¸€é—®é¢˜çš„å›ç­”å¤šæ ·æ€§

        Args:
            test_questions: æµ‹è¯•é—®é¢˜åˆ—è¡¨
            num_samples: æ¯ä¸ªé—®é¢˜ç”Ÿæˆæ¬¡æ•°

        Returns:
            å¤šæ ·æ€§è¯„ä¼°ç»“æœ
        """
        print("=" * 70)
        print("ğŸ¨ å¤šæ ·æ€§è¯„ä¼°")
        print("=" * 70)
        print(f"æ¯ä¸ªé—®é¢˜ç”Ÿæˆ {num_samples} æ¬¡å›ç­”ï¼ˆæ¸©åº¦=0.7ï¼‰\n")

        diversity_scores = []

        for i, question in enumerate(test_questions, 1):
            print(f"[{i}/{len(test_questions)}] é—®é¢˜: {question}")

            # ç”Ÿæˆå¤šä¸ªå›ç­”
            responses = []
            for j in range(num_samples):
                response = self.inferencer.generate(
                    question,
                    max_new_tokens=200,
                    temperature=0.7,  # æé«˜æ¸©åº¦ä»¥å¢åŠ å¤šæ ·æ€§
                    top_p=0.95,
                    repetition_penalty=1.0,
                )
                responses.append(response)
                print(f"  å›ç­”{j+1}: {response[:100]}...")

            # è®¡ç®—å¤šæ ·æ€§ï¼ˆç®€å•æ–¹æ³•ï¼šæ¯”è¾ƒå›ç­”é•¿åº¦å·®å¼‚ï¼‰
            lengths = [len(r) for r in responses]
            avg_length = sum(lengths) / len(lengths)
            length_variance = sum((l - avg_length) ** 2 for l in lengths) / len(lengths)

            diversity_score = min(length_variance / 1000, 1.0)  # å½’ä¸€åŒ–åˆ°[0,1]
            diversity_scores.append(diversity_score)

            print(f"  å¤šæ ·æ€§å¾—åˆ†: {diversity_score:.3f}")
            print("-" * 70)

        avg_diversity = sum(diversity_scores) / len(diversity_scores)

        print("\n" + "=" * 70)
        print("ğŸ¨ å¤šæ ·æ€§è¯„ä¼°æ€»ç»“")
        print("=" * 70)
        print(f"å¹³å‡å¤šæ ·æ€§å¾—åˆ†: {avg_diversity:.3f}")
        print(f"å¾—åˆ†è¯´æ˜: 0=å›ç­”å®Œå…¨ç›¸åŒ, 1=å›ç­”å·®å¼‚å¾ˆå¤§")
        print("=" * 70 + "\n")

        return {
            "type": "diversity",
            "avg_diversity": avg_diversity,
            "scores": diversity_scores
        }

    def evaluate_consistency(
        self,
        test_questions: list,
        num_samples: int = 3
    ) -> dict:
        """
        ä¸€è‡´æ€§è¯„ä¼° - æµ‹è¯•æ¨¡å‹å¯¹åŒä¸€é—®é¢˜å›ç­”çš„ç¨³å®šæ€§

        Args:
            test_questions: æµ‹è¯•é—®é¢˜åˆ—è¡¨
            num_samples: æ¯ä¸ªé—®é¢˜ç”Ÿæˆæ¬¡æ•°

        Returns:
            ä¸€è‡´æ€§è¯„ä¼°ç»“æœ
        """
        print("=" * 70)
        print("ğŸ¯ ä¸€è‡´æ€§è¯„ä¼°")
        print("=" * 70)
        print(f"æ¯ä¸ªé—®é¢˜ç”Ÿæˆ {num_samples} æ¬¡å›ç­”ï¼ˆæ¸©åº¦=0.1ï¼‰\n")

        consistency_scores = []

        for i, question in enumerate(test_questions, 1):
            print(f"[{i}/{len(test_questions)}] é—®é¢˜: {question}")

            # ç”Ÿæˆå¤šä¸ªå›ç­”ï¼ˆä½¿ç”¨ä½æ¸©åº¦ï¼‰
            responses = []
            for j in range(num_samples):
                response = self.inferencer.generate(
                    question,
                    max_new_tokens=200,
                    temperature=0.1,  # ä½æ¸©åº¦ä»¥ä¿æŒä¸€è‡´æ€§
                    top_p=0.95,
                    repetition_penalty=1.0,
                )
                responses.append(response)
                print(f"  å›ç­”{j+1}: {response[:100]}...")

            # è®¡ç®—ä¸€è‡´æ€§ï¼ˆç®€å•æ–¹æ³•ï¼šæ¯”è¾ƒå›ç­”çš„ç›¸ä¼¼åº¦ï¼‰
            # è¿™é‡Œä½¿ç”¨å…³é”®è¯ç›¸ä¼¼åº¦
            all_words = set()
            for r in responses:
                all_words.update(r.split())

            similarities = []
            for word in all_words:
                count = sum(1 for r in responses if word in r)
                if count == num_samples:  # æ‰€æœ‰å›ç­”éƒ½åŒ…å«è¿™ä¸ªè¯
                    similarities.append(1)
                elif count > 0:
                    similarities.append(count / num_samples)

            consistency = sum(similarities) / len(similarities) if similarities else 0
            consistency_scores.append(consistency)

            print(f"  ä¸€è‡´æ€§å¾—åˆ†: {consistency:.3f}")
            print("-" * 70)

        avg_consistency = sum(consistency_scores) / len(consistency_scores)

        print("\n" + "=" * 70)
        print("ğŸ¯ ä¸€è‡´æ€§è¯„ä¼°æ€»ç»“")
        print("=" * 70)
        print(f"å¹³å‡ä¸€è‡´æ€§å¾—åˆ†: {avg_consistency:.3f}")
        print(f"å¾—åˆ†è¯´æ˜: 0=å®Œå…¨ä¸ä¸€è‡´, 1=å®Œå…¨ä¸€è‡´")
        print("=" * 70 + "\n")

        return {
            "type": "consistency",
            "avg_consistency": avg_consistency,
            "scores": consistency_scores
        }

    def evaluate_on_test_set(
        self,
        test_data_file: str,
        sample_size: int = 20
    ) -> dict:
        """
        åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°

        Args:
            test_data_file: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSONLæ ¼å¼ï¼‰
            sample_size: é‡‡æ ·æ•°é‡

        Returns:
            æµ‹è¯•é›†è¯„ä¼°ç»“æœ
        """
        print("=" * 70)
        print("ğŸ“š æµ‹è¯•é›†è¯„ä¼°")
        print("=" * 70)

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data_path = Path(test_data_file)
        if not test_data_path.exists():
            print(f"âš ï¸  æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_data_path}")
            return {}

        with open(test_data_path, 'r', encoding='utf-8') as f:
            all_data = [json.loads(line.strip()) for line in f if line.strip()]

        # éšæœºé‡‡æ ·
        if len(all_data) > sample_size:
            sampled_data = random.sample(all_data, sample_size)
        else:
            sampled_data = all_data

        print(f"ä» {len(all_data)} æ¡æ•°æ®ä¸­é‡‡æ · {len(sampled_data)} æ¡\n")

        correct_count = 0
        for i, item in enumerate(sampled_data, 1):
            question = item.get("instruction", "")
            expected_answer = item.get("output", "")

            print(f"[{i}/{len(sampled_data)}] {question[:60]}...")

            # ç”Ÿæˆå›ç­”
            response = self.inferencer.generate(
                question,
                max_new_tokens=300,
                temperature=0.1,
            )

            # ç®€å•ç›¸ä¼¼åº¦è¯„ä¼°ï¼ˆåŸºäºè¯é‡å ï¼‰
            response_words = set(response.split())
            expected_words = set(expected_answer.split())

            overlap = len(response_words & expected_words)
            union = len(response_words | expected_words)
            similarity = overlap / union if union > 0 else 0

            print(f"  é¢„æœŸ: {expected_answer[:80]}...")
            print(f"  å®é™…: {response[:80]}...")
            print(f"  ç›¸ä¼¼åº¦: {similarity:.3f}")

            if similarity >= 0.3:  # é˜ˆå€¼å¯è°ƒæ•´
                correct_count += 1
                print(f"  âœ… åˆæ ¼")
            else:
                print(f"  âŒ ä¸åˆæ ¼")

            print("-" * 70)

        accuracy = correct_count / len(sampled_data)

        print("\n" + "=" * 70)
        print("ğŸ“š æµ‹è¯•é›†è¯„ä¼°æ€»ç»“")
        print("=" * 70)
        print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(sampled_data)}")
        print(f"åˆæ ¼æ•°: {correct_count}")
        print(f"åˆæ ¼ç‡: {accuracy*100:.1f}%")
        print("=" * 70 + "\n")

        return {
            "type": "test_set",
            "sample_size": len(sampled_data),
            "correct": correct_count,
            "accuracy": accuracy
        }

    def save_report(self, results: list, output_file: str = None):
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"evaluation_report_{timestamp}.json"

        report = {
            "timestamp": datetime.now().isoformat(),
            "model_path": self.model_path,
            "base_model": self.base_model,
            "evaluations": results
        }

        output_path = Path(output_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {output_path}")


def main():
    """ä¸»è¯„ä¼°æµç¨‹"""
    print("=" * 70)
    print("ğŸ”¬ æ¨¡å‹æ•ˆæœè¯„ä¼°ç³»ç»Ÿ")
    print("=" * 70)
    print()

    # æ¨¡å‹é…ç½®
    model_path = "outputs/qwen2_5-3b-trained"
    base_model = "models/Qwen/Qwen2.5-3B"

    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = ModelEvaluator(model_path, base_model)
    evaluator.load_model()

    all_results = []

    # 1. ç²¾ç¡®åº¦è¯„ä¼°
    accuracy_test_cases = [
        {
            "question": "å¯¹äºè¯¾é¢˜åä½œè´¹ã€åˆ¶ä½œè´¹ã€ææ–™è´¹ç­‰è´¹ç”¨ï¼Œåä¸œå¸ˆèŒƒå¤§å­¦å¯¹åè®®å’ŒåˆåŒçš„ç­¾è®¢é‡‘é¢æœ‰ä»€ä¹ˆå…·ä½“è¦æ±‚ï¼Ÿ",
            "keywords": ["3000", "10000", "åè®®", "åˆåŒ"],
            "description": "é‡‘é¢é˜ˆå€¼æµ‹è¯•"
        },
        {
            "question": "åä¸œå¸ˆèŒƒå¤§å­¦å¯¹åŠå…¬ç”¨å“çš„æŠ¥é”€æœ‰å“ªäº›ç‰¹æ®Šè§„å®šï¼Ÿ",
            "keywords": ["500", "æ˜ç»†æ¸…å•", "å‘ç¥¨"],
            "description": "åŠå…¬ç”¨å“è§„å®šæµ‹è¯•"
        },
        {
            "question": "å·®æ—…è´¹çš„æŠ¥é”€éœ€è¦æä¾›å“ªäº›ææ–™ï¼Ÿ",
            "keywords": ["å®¡æ‰¹å•", "æœºç¥¨", "å‘ç¥¨", "ä½å®¿"],
            "description": "å·®æ—…è´¹æµç¨‹æµ‹è¯•"
        },
        {
            "question": "å›¾ä¹¦èµ„æ–™æŠ¥é”€åœ¨ä»€ä¹ˆé‡‘é¢ä»¥ä¸Šéœ€è¦é™„åˆåŒï¼Ÿ",
            "keywords": ["30000", "åˆåŒ", "å›¾ä¹¦"],
            "description": "å›¾ä¹¦é‡‡è´­æµ‹è¯•"
        },
        {
            "question": "ä¼šè®®è´¹æŠ¥é”€éœ€è¦æä¾›å“ªäº›å®¡æ‰¹ææ–™ï¼Ÿ",
            "keywords": ["ä¼šè®®é€šçŸ¥", "ç­¾åˆ°", "å®¡æ‰¹"],
            "description": "ä¼šè®®è´¹æµ‹è¯•"
        },
    ]

    accuracy_result = evaluator.evaluate_accuracy(accuracy_test_cases)
    all_results.append(accuracy_result)

    # 2. å¤šæ ·æ€§è¯„ä¼°
    diversity_questions = [
        "è¯·è¯´æ˜å·®æ—…è´¹çš„æŠ¥é”€æµç¨‹",
        "åä¸œå¸ˆèŒƒå¤§å­¦å¯¹åŠå…¬ç”¨å“æŠ¥é”€æœ‰ä»€ä¹ˆè§„å®šï¼Ÿ"
    ]

    diversity_result = evaluator.evaluate_diversity(diversity_questions, num_samples=3)
    all_results.append(diversity_result)

    # 3. ä¸€è‡´æ€§è¯„ä¼°
    consistency_questions = [
        "è¯¾é¢˜åä½œè´¹åœ¨ä»€ä¹ˆé‡‘é¢ä»¥ä¸Šéœ€è¦ç­¾è®¢åˆåŒï¼Ÿ",
        "åŠå…¬ç”¨å“æŠ¥é”€éœ€è¦æä¾›ä»€ä¹ˆææ–™ï¼Ÿ"
    ]

    consistency_result = evaluator.evaluate_consistency(consistency_questions, num_samples=3)
    all_results.append(consistency_result)

    # 4. æµ‹è¯•é›†è¯„ä¼°
    test_data_file = "data/æŠ¥é”€ç»†åˆ™_distilled_chunked.jsonl"
    test_set_result = evaluator.evaluate_on_test_set(test_data_file, sample_size=20)
    if test_set_result:
        all_results.append(test_set_result)

    # ä¿å­˜æŠ¥å‘Š
    evaluator.save_report(all_results)

    # æ€»ä½“è¯„åˆ†
    print("\n" + "=" * 70)
    print("ğŸ¯ æ€»ä½“è¯„ä¼°æŠ¥å‘Š")
    print("=" * 70)

    if len(all_results) > 0:
        accuracy = all_results[0]["accuracy"]
        diversity = all_results[1]["avg_diversity"]
        consistency = all_results[2]["avg_consistency"]

        print(f"\nğŸ“Š æ ¸å¿ƒæŒ‡æ ‡:")
        print(f"  ç²¾ç¡®åº¦: {accuracy*100:.1f}%")
        print(f"  å¤šæ ·æ€§: {diversity:.3f}")
        print(f"  ä¸€è‡´æ€§: {consistency:.3f}")

        # ç»¼åˆè¯„åˆ†
        overall_score = (accuracy * 0.6 + consistency * 0.3 + diversity * 0.1)
        print(f"\nğŸ† ç»¼åˆè¯„åˆ†: {overall_score*100:.1f}/100")

        if overall_score >= 0.8:
            print(f"  è¯„çº§: â­â­â­â­â­ ä¼˜ç§€")
        elif overall_score >= 0.7:
            print(f"  è¯„çº§: â­â­â­â­ è‰¯å¥½")
        elif overall_score >= 0.6:
            print(f"  è¯„çº§: â­â­â­ åˆæ ¼")
        else:
            print(f"  è¯„çº§: â­â­ éœ€æ”¹è¿›")

    print("=" * 70 + "\n")

    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
