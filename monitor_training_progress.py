#!/usr/bin/env python3
"""
è®­ç»ƒè¿›åº¦ç›‘æ§å·¥å…·
ç”¨äºç›‘æ§å’Œåˆ†ææ¨¡å‹è®­ç»ƒè¿›åº¦ï¼ŒåŒ…æ‹¬æŸå¤±å¯è§†åŒ–ã€è®­ç»ƒçŠ¶æ€æ‘˜è¦ç­‰åŠŸèƒ½
"""

import os
import json
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime
import pandas as pd

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams["font.sans-serif"] = ["SimHei", "Arial Unicode MS", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜


def load_checkpoint_data(checkpoint_dir: str) -> Dict:
    """
    åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®

    Args:
        checkpoint_dir: æ£€æŸ¥ç‚¹ç›®å½•è·¯å¾„

    Returns:
        åŒ…å«è®­ç»ƒçŠ¶æ€æ•°æ®çš„å­—å…¸
    """
    checkpoint_path = Path(checkpoint_dir)
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨: {checkpoint_dir}")

    # åŠ è½½trainer_state.json
    trainer_state_path = checkpoint_path / "trainer_state.json"
    if not trainer_state_path.exists():
        raise FileNotFoundError(f"trainer_state.jsonæ–‡ä»¶ä¸å­˜åœ¨: {trainer_state_path}")

    with open(trainer_state_path, "r", encoding="utf-8") as f:
        trainer_state = json.load(f)

    return trainer_state


def extract_training_metrics(trainer_state: Dict) -> Tuple[List, List, List, List]:
    """
    ä»è®­ç»ƒçŠ¶æ€ä¸­æå–æŒ‡æ ‡

    Args:
        trainer_state: è®­ç»ƒçŠ¶æ€å­—å…¸

    Returns:
        (steps, losses, learning_rates, grad_norms): è®­ç»ƒæ­¥éª¤ã€æŸå¤±å€¼ã€å­¦ä¹ ç‡å’Œæ¢¯åº¦èŒƒæ•°
    """
    log_history = trainer_state.get("log_history", [])

    steps = []
    losses = []
    learning_rates = []
    grad_norms = []

    for entry in log_history:
        if "loss" in entry:
            steps.append(entry.get("step", 0))
            losses.append(entry["loss"])
            learning_rates.append(entry.get("learning_rate", 0))
            grad_norms.append(entry.get("grad_norm", 0))

    return steps, losses, learning_rates, grad_norms


def plot_training_progress(
    steps: List,
    losses: List,
    learning_rates: List,
    grad_norms: List,
    save_path: Optional[str] = None,
    show_plot: bool = True,
):
    """
    ç»˜åˆ¶è®­ç»ƒè¿›åº¦å›¾è¡¨

    Args:
        steps: è®­ç»ƒæ­¥éª¤åˆ—è¡¨
        losses: æŸå¤±å€¼åˆ—è¡¨
        learning_rates: å­¦ä¹ ç‡åˆ—è¡¨
        grad_norms: æ¢¯åº¦èŒƒæ•°åˆ—è¡¨
        save_path: å›¾è¡¨ä¿å­˜è·¯å¾„
        show_plot: æ˜¯å¦æ˜¾ç¤ºå›¾è¡¨
    """
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

    # æŸå¤±æ›²çº¿
    ax1.plot(steps, losses, "b-", linewidth=2)
    ax1.set_title("è®­ç»ƒæŸå¤±", fontsize=14, fontweight="bold")
    ax1.set_xlabel("è®­ç»ƒæ­¥éª¤")
    ax1.set_ylabel("æŸå¤±å€¼")
    ax1.grid(True, alpha=0.3)

    # æŸå¤±ç§»åŠ¨å¹³å‡
    if len(losses) > 10:
        window_size = min(10, len(losses) // 10)
        moving_avg = pd.Series(losses).rolling(window=window_size).mean()
        ax1.plot(
            steps, moving_avg, "r-", linewidth=2, label=f"ç§»åŠ¨å¹³å‡(çª—å£={window_size})"
        )
        ax1.legend()

    # å­¦ä¹ ç‡æ›²çº¿
    ax2.plot(steps, learning_rates, "g-", linewidth=2)
    ax2.set_title("å­¦ä¹ ç‡å˜åŒ–", fontsize=14, fontweight="bold")
    ax2.set_xlabel("è®­ç»ƒæ­¥éª¤")
    ax2.set_ylabel("å­¦ä¹ ç‡")
    ax2.grid(True, alpha=0.3)

    # æ¢¯åº¦èŒƒæ•°æ›²çº¿
    ax3.plot(steps, grad_norms, "orange", linewidth=2)
    ax3.set_title("æ¢¯åº¦èŒƒæ•°", fontsize=14, fontweight="bold")
    ax3.set_xlabel("è®­ç»ƒæ­¥éª¤")
    ax3.set_ylabel("æ¢¯åº¦èŒƒæ•°")
    ax3.grid(True, alpha=0.3)
    ax3.set_yscale("log")  # ä½¿ç”¨å¯¹æ•°åˆ»åº¦ï¼Œå› ä¸ºæ¢¯åº¦èŒƒæ•°å˜åŒ–å¯èƒ½å¾ˆå¤§

    # æŸå¤±åˆ†å¸ƒç›´æ–¹å›¾
    ax4.hist(losses, bins=30, alpha=0.7, color="skyblue", edgecolor="black")
    ax4.set_title("æŸå¤±å€¼åˆ†å¸ƒ", fontsize=14, fontweight="bold")
    ax4.set_xlabel("æŸå¤±å€¼")
    ax4.set_ylabel("é¢‘æ¬¡")
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"Chart saved to: {save_path}")

    if show_plot:
        plt.show()


def generate_training_summary(trainer_state: Dict) -> Dict:
    """
    ç”Ÿæˆè®­ç»ƒæ‘˜è¦

    Args:
        trainer_state: è®­ç»ƒçŠ¶æ€å­—å…¸

    Returns:
        åŒ…å«è®­ç»ƒæ‘˜è¦çš„å­—å…¸
    """
    log_history = trainer_state.get("log_history", [])

    # æå–åŸºæœ¬ä¿¡æ¯
    current_epoch = trainer_state.get("epoch", 0)
    global_step = trainer_state.get("global_step", 0)
    max_steps = trainer_state.get("max_steps", 0)
    num_train_epochs = trainer_state.get("num_train_epochs", 0)

    # è®¡ç®—è¿›åº¦
    epoch_progress = (
        (current_epoch / num_train_epochs) * 100 if num_train_epochs > 0 else 0
    )
    step_progress = (global_step / max_steps) * 100 if max_steps > 0 else 0

    # æå–æŸå¤±ä¿¡æ¯
    losses = [entry.get("loss", 0) for entry in log_history if "loss" in entry]
    if losses:
        current_loss = losses[-1]
        min_loss = min(losses)
        max_loss = max(losses)
        avg_loss = sum(losses) / len(losses)

        # è®¡ç®—æŸå¤±å˜åŒ–è¶‹åŠ¿
        if len(losses) >= 10:
            recent_losses = losses[-10:]
            earlier_losses = (
                losses[-20:-10] if len(losses) >= 20 else losses[: len(losses) - 10]
            )
            recent_avg = sum(recent_losses) / len(recent_losses)
            earlier_avg = sum(earlier_losses) / len(earlier_losses)
            loss_trend = "ä¸‹é™" if recent_avg < earlier_avg else "ä¸Šå‡"
        else:
            loss_trend = "æ•°æ®ä¸è¶³"
    else:
        current_loss = min_loss = max_loss = avg_loss = 0
        loss_trend = "æ— æ•°æ®"

    # æå–å­¦ä¹ ç‡ä¿¡æ¯
    learning_rates = [
        entry.get("learning_rate", 0)
        for entry in log_history
        if "learning_rate" in entry
    ]
    current_lr = learning_rates[-1] if learning_rates else 0

    # æå–æ¢¯åº¦èŒƒæ•°ä¿¡æ¯
    grad_norms = [
        entry.get("grad_norm", 0) for entry in log_history if "grad_norm" in entry
    ]
    current_grad_norm = grad_norms[-1] if grad_norms else 0

    summary = {
        "è®­ç»ƒè¿›åº¦": {
            "å½“å‰è½®æ¬¡": f"{current_epoch:.2f} / {num_train_epochs}",
            "è½®æ¬¡è¿›åº¦": f"{epoch_progress:.2f}%",
            "å½“å‰æ­¥éª¤": f"{global_step} / {max_steps}",
            "æ­¥éª¤è¿›åº¦": f"{step_progress:.2f}%",
        },
        "æŸå¤±ä¿¡æ¯": {
            "å½“å‰æŸå¤±": f"{current_loss:.6f}",
            "æœ€å°æŸå¤±": f"{min_loss:.6f}",
            "æœ€å¤§æŸå¤±": f"{max_loss:.6f}",
            "å¹³å‡æŸå¤±": f"{avg_loss:.6f}",
            "æŸå¤±è¶‹åŠ¿": loss_trend,
        },
        "è®­ç»ƒå‚æ•°": {
            "å½“å‰å­¦ä¹ ç‡": f"{current_lr:.2e}",
            "å½“å‰æ¢¯åº¦èŒƒæ•°": f"{current_grad_norm:.4f}",
            "æ€»è®­ç»ƒæ­¥æ•°": max_steps,
            "è®­ç»ƒè½®æ¬¡": num_train_epochs,
        },
        "æ—¶é—´ä¿¡æ¯": {"æ£€æŸ¥ç‚¹æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")},
    }

    return summary


def print_training_summary(summary: Dict):
    """
    æ‰“å°è®­ç»ƒæ‘˜è¦

    Args:
        summary: è®­ç»ƒæ‘˜è¦å­—å…¸
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š è®­ç»ƒè¿›åº¦æ‘˜è¦")
    print("=" * 60)

    for category, items in summary.items():
        print(f"\nğŸ”¸ {category}:")
        for key, value in items.items():
            print(f"  {key}: {value}")

    print("\n" + "=" * 60)


def find_latest_checkpoint(outputs_dir: str = "./outputs") -> Optional[str]:
    """
    æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹ç›®å½•

    Args:
        outputs_dir: è¾“å‡ºç›®å½•è·¯å¾„

    Returns:
        æœ€æ–°æ£€æŸ¥ç‚¹ç›®å½•è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
    """
    outputs_path = Path(outputs_dir)
    if not outputs_path.exists():
        return None

    # æŸ¥æ‰¾æ‰€æœ‰checkpointç›®å½•
    checkpoint_dirs = []
    for item in outputs_path.iterdir():
        if item.is_dir() and item.name.startswith("checkpoint-"):
            try:
                step_num = int(item.name.split("-")[1])
                checkpoint_dirs.append((step_num, str(item)))
            except (IndexError, ValueError):
                continue

    if not checkpoint_dirs:
        return None

    # è¿”å›æ­¥éª¤æ•°æœ€å¤§çš„æ£€æŸ¥ç‚¹
    latest_checkpoint = max(checkpoint_dirs, key=lambda x: x[0])[1]
    return latest_checkpoint


def compare_checkpoints(checkpoint_dirs: List[str], save_path: Optional[str] = None):
    """
    æ¯”è¾ƒå¤šä¸ªæ£€æŸ¥ç‚¹çš„è®­ç»ƒè¿›åº¦

    Args:
        checkpoint_dirs: æ£€æŸ¥ç‚¹ç›®å½•åˆ—è¡¨
        save_path: å›¾è¡¨ä¿å­˜è·¯å¾„
    """
    plt.figure(figsize=(15, 8))

    for checkpoint_dir in checkpoint_dirs:
        try:
            trainer_state = load_checkpoint_data(checkpoint_dir)
            steps, losses, _, _ = extract_training_metrics(trainer_state)

            # ä½¿ç”¨æ£€æŸ¥ç‚¹åç§°ä½œä¸ºæ ‡ç­¾
            label = Path(checkpoint_dir).name
            plt.plot(steps, losses, label=label, linewidth=2, marker="o", markersize=3)
        except Exception as e:
            print(f"åŠ è½½æ£€æŸ¥ç‚¹ {checkpoint_dir} å¤±è´¥: {e}")

    plt.title("å¤šæ£€æŸ¥ç‚¹æŸå¤±æ¯”è¾ƒ", fontsize=16, fontweight="bold")
    plt.xlabel("è®­ç»ƒæ­¥éª¤")
    plt.ylabel("æŸå¤±å€¼")
    plt.legend()
    plt.grid(True, alpha=0.3)

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"Comparison chart saved to: {save_path}")

    plt.show()


def main():
    parser = argparse.ArgumentParser(description="è®­ç»ƒè¿›åº¦ç›‘æ§å·¥å…·")
    parser.add_argument("--checkpoint", "-c", type=str, help="æ£€æŸ¥ç‚¹ç›®å½•è·¯å¾„")
    parser.add_argument(
        "--outputs", "-o", type=str, default="./outputs", help="è¾“å‡ºç›®å½•è·¯å¾„"
    )
    parser.add_argument("--plot", "-p", action="store_true", help="æ˜¾ç¤ºè®­ç»ƒè¿›åº¦å›¾è¡¨")
    parser.add_argument("--save-plot", "-s", type=str, help="ä¿å­˜å›¾è¡¨åˆ°æŒ‡å®šè·¯å¾„")
    parser.add_argument("--summary", "-u", action="store_true", help="æ˜¾ç¤ºè®­ç»ƒæ‘˜è¦")
    parser.add_argument("--compare", type=str, nargs="+", help="æ¯”è¾ƒå¤šä¸ªæ£€æŸ¥ç‚¹")
    parser.add_argument("--latest", "-l", action="store_true", help="ä½¿ç”¨æœ€æ–°æ£€æŸ¥ç‚¹")

    args = parser.parse_args()

    # å¦‚æœæŒ‡å®šäº†--latestæˆ–æ²¡æœ‰æŒ‡å®šæ£€æŸ¥ç‚¹ï¼Œåˆ™æŸ¥æ‰¾æœ€æ–°æ£€æŸ¥ç‚¹
    if args.latest or not args.checkpoint:
        latest_checkpoint = find_latest_checkpoint(args.outputs)
        if latest_checkpoint:
            args.checkpoint = latest_checkpoint
            print(f"Using latest checkpoint: {latest_checkpoint}")
        else:
            print(
                "No checkpoint found, please specify checkpoint path or ensure checkpoints exist in outputs directory"
            )
            return

    # æ¯”è¾ƒå¤šä¸ªæ£€æŸ¥ç‚¹
    if args.compare:
        compare_checkpoints(args.compare, args.save_plot)
        return

    # åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®
    try:
        trainer_state = load_checkpoint_data(args.checkpoint)
    except Exception as e:
        print(f"Failed to load checkpoint: {e}")
        return

    # æå–è®­ç»ƒæŒ‡æ ‡
    steps, losses, learning_rates, grad_norms = extract_training_metrics(trainer_state)

    # æ˜¾ç¤ºè®­ç»ƒæ‘˜è¦
    if args.summary or (not args.plot and not args.save_plot):
        summary = generate_training_summary(trainer_state)
        print_training_summary(summary)

    # ç»˜åˆ¶è®­ç»ƒè¿›åº¦å›¾è¡¨
    if args.plot or args.save_plot:
        plot_training_progress(
            steps,
            losses,
            learning_rates,
            grad_norms,
            save_path=args.save_plot,
            show_plot=args.plot,
        )


if __name__ == "__main__":
    main()
