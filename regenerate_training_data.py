#!/usr/bin/env python3
"""
é‡æ–°ç”Ÿæˆæ›´å¤šè®­ç»ƒæ•°æ®
é€šè¿‡å¢åŠ  num_pairs å‚æ•°æ¥ç”Ÿæˆæ›´å¤šé—®ç­”å¯¹
"""

import sys
from pathlib import Path

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from data_prep import DataDistiller


def regenerate_data():
    """é‡æ–°ç”Ÿæˆè®­ç»ƒæ•°æ® - ä½¿ç”¨ä¼˜åŒ–åçš„é…ç½®"""

    print("=" * 70)
    print("ğŸ”„ é‡æ–°ç”Ÿæˆè®­ç»ƒæ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬")
    print("=" * 70)
    print("   âœ… å¼ºè°ƒæ•°å­—ç±»ä¿¡æ¯æå–")
    print("   âœ… å¢åŠ æ ·æœ¬å¯†åº¦ (15å¯¹/å—)")
    print("   âœ… ä¼˜åŒ–æç¤ºè¯")

    # åŸå§‹PDFæ–‡æ¡£
    pdf_path = "docs/æŠ¥é”€ç»†åˆ™.pdf"

    # è¾“å‡ºè·¯å¾„
    output_path = "data/æŠ¥é”€ç»†åˆ™_distilled_optimized.jsonl"

    # æ•°æ®è’¸é¦å™¨
    distiller = DataDistiller()

    print(f"\næ­£åœ¨å¤„ç†æ–‡æ¡£: {pdf_path}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"ç”Ÿæˆç­–ç•¥: ä½¿ç”¨ä¼˜åŒ–åçš„è’¸é¦æç¤ºè¯\n")

    # æ™ºèƒ½åˆ†å—å¤„ç†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    # ç›®æ ‡ï¼šç”Ÿæˆé«˜è´¨é‡ã€å¼ºè°ƒæ•°å­—çš„è®­ç»ƒæ•°æ®
    # ç­–ç•¥ï¼šå‡å°chunk_sizeä»¥äº§ç”Ÿæ›´å¤šå—ï¼Œä½¿ç”¨æ™ºè°±AIï¼ˆä¼˜åŒ–åæç¤ºè¯ï¼‰

    target_pairs = 180  # ç›®æ ‡ç”Ÿæˆ180æ¡ï¼ˆä½¿ç”¨æ™ºè°±AIï¼‰
    pairs_per_chunk = 15  # æ¯ä¸ªchunkç”Ÿæˆ15æ¡ï¼ˆä¼˜åŒ–åçš„æ¨èå€¼ï¼‰
    chunk_size = 5000  # chunkå¤§å°5000å­—ç¬¦ï¼ˆå‡å°ä»¥äº§ç”Ÿæ›´å¤šå—ï¼Œçº¦11-12å—ï¼‰
    overlap = 300  # é‡å 300å­—ç¬¦

    print(f"å‚æ•°é…ç½®:")
    print(f"  ç›®æ ‡ç”Ÿæˆæ•°é‡: {target_pairs}")
    print(f"  æ¯å—ç”Ÿæˆé—®ç­”: {pairs_per_chunk} (ä¼˜åŒ–å)")
    print(f"  åˆ†å—å¤§å°: {chunk_size} å­—ç¬¦ (å‡å°ä»¥å¢åŠ å—æ•°)")
    print(f"  é‡å å¤§å°: {overlap} å­—ç¬¦")
    print(f"  ä½¿ç”¨API: æ™ºè°±AI GLM-4-Flash (ä¼˜åŒ–æç¤ºè¯)")

    try:
        # æ‰§è¡Œåˆ†å—å¤„ç†
        result_path = distiller.process_file_chunked(
            file_path=pdf_path,
            output_dir=str(Path(output_path).parent),
            num_pairs_per_chunk=pairs_per_chunk,
            chunk_size=chunk_size,
            overlap=overlap,
        )

        # ç»Ÿè®¡ç”Ÿæˆçš„æ•°æ®é‡
        import json
        data_count = 0
        with open(result_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data_count += 1

        print("\n" + "=" * 70)
        print("âœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼")
        print("=" * 70)
        print(f"\nåŸå§‹æ•°æ®é‡: 863 æ¡")
        print(f"æ–°ç”Ÿæˆæ•°æ®é‡: {data_count} æ¡")
        if data_count > 863:
            print(f"å¢åŠ : {data_count - 863} æ¡ ({(data_count - 863) / 863 * 100:.1f}%)")
        print(f"\næ•°æ®æ–‡ä»¶: {result_path}")
        print(f"\nğŸ’¡ ä¼˜åŒ–å†…å®¹:")
        print(f"   - å¼ºè°ƒæ•°å­—ç±»ä¿¡æ¯æå–ï¼ˆ30%+æ•°å­—é—®é¢˜ï¼‰")
        print(f"   - å¢åŠ æ ·æœ¬å¯†åº¦ï¼ˆ15å¯¹/å—ï¼‰")
        print(f"   - ä¼˜åŒ–è’¸é¦æç¤ºè¯")

        # æ˜¾ç¤ºå‰3æ¡é¢„è§ˆ
        print("\n" + "=" * 70)
        print("æ•°æ®é¢„è§ˆï¼ˆå‰3æ¡ï¼‰:")
        print("=" * 70)
        with open(result_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= 3:
                    break
                data = json.loads(line)
                print(f"\nã€ç¬¬{i+1}æ¡ã€‘")
                print(f"é—®é¢˜: {data['instruction'][:80]}...")
                print(f"å›ç­”: {data['output'][:100]}...")

        print("\n" + "=" * 70)
        print("ä¸‹ä¸€æ­¥ï¼šä½¿ç”¨æ–°æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹")
        print("=" * 70)
        print("\nå‘½ä»¤ï¼š")
        print("  python3 train_with_new_data.py")

        return result_path

    except Exception as e:
        print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    regenerate_data()
