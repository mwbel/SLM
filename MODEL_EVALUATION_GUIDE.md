# 模型效果检验指南

本指南提供了完整的模型效果检验方法和工具。

## 📊 评估结果概览

### 当前模型表现
**模型**: Qwen2.5-3B (LoRA微调)
**训练数据**: 863条财务报销问答对
**综合评分**: **65/100** (⭐⭐⭐ 合格)

### 核心指标
| 指标 | 得分 | 说明 |
|------|------|------|
| **精确度** | 80.0% | 关键词匹配合格率 |
| **多样性** | 0.200 | 回答差异性 (0-1) |
| **一致性** | 0.500 | 回答稳定性 (0-1) |
| **平均关键词覆盖率** | 63.3% | 预期信息的完整性 |

---

## 🛠️ 评估工具使用指南

### 1️⃣ 自动化批量评估 (`evaluate_model.py`)

**用途**: 全面评估模型在多个维度上的表现

**运行**:
```bash
python3 evaluate_model.py
```

**评估维度**:
- ✅ **精确度评估**: 基于关键词匹配测试准确性
- ✅ **多样性评估**: 测试模型回答的差异性
- ✅ **一致性评估**: 测试模型回答的稳定性
- ✅ **测试集评估**: 在真实数据上抽样测试

**输出**:
- 控制台显示详细测试过程
- 自动生成JSON报告 (`evaluation_report_YYYYMMDD_HHMMSS.json`)

**优点**:
- 全面、自动化
- 可重复、可量化
- 适合回归测试

**缺点**:
- 需要预先设计测试用例
- 关键词匹配可能不够智能

---

### 2️⃣ 对比测试 (`compare_models.py`)

**用途**: 对比基座模型和训练后模型的差异，验证训练效果

**运行**:
```bash
python3 compare_models.py
```

**功能**:
- 🔵 同时加载基座模型和训练后模型
- ⚖️ 对比同一问题的回答差异
- 📊 统计哪个模型表现更好
- 🔍 深度分析单个问题的差异

**输出示例**:
```
🔵 基座模型回答:
  华东师范大学...

🟢 训练后模型回答:
  差费报销需提供出差审批单、机票、车票、住宿发票等原始凭证。

✅ 训练后模型在关键词匹配上表现更好
```

**使用场景**:
- 验证训练是否有效
- 分析训练带来的变化
- 调优训练参数

---

### 3️⃣ 交互式人工评估 (`interactive_eval.py`)

**用途**: 手动测试模型，支持实时对话

**运行**:
```bash
python3 interactive_eval.py
```

**命令**:
- `<直接输入>` - 提问
- `/t 0.5` - 设置温度参数
- `/p` - 切换提示格式
- `/stats` - 查看统计信息
- `/s` - 保存对话历史
- `/q` - 退出

**特点**:
- 实时交互
- 支持多轮对话
- 可调整生成参数
- 保存对话记录

**优点**:
- 直观、灵活
- 适合探索性测试
- 可以发现自动化测试遗漏的问题

**缺点**:
- 不可重复
- 难以量化
- 需要人工判断

---

## 📋 评估指标详解

### 1. 精确度 (Accuracy)

**定义**: 模型回答包含预期关键词的比例

**计算方法**:
```
关键词覆盖率 = (找到的关键词数 / 预期关键词总数)
合格 = 关键词覆盖率 >= 50%
精确度 = (合格用例数 / 总用例数)
```

**示例**:
```python
预期关键词: ["3000", "10000", "协议", "合同"]
模型回答: "...协议和合同..."
找到关键词: ["协议", "合同"]
覆盖率: 2/4 = 50%
结果: ✅ 合格
```

**优缺点**:
- ✅ 简单、可量化
- ⚠️ 只能评估事实性信息
- ⚠️ 不评估语义理解

---

### 2. 多样性 (Diversity)

**定义**: 模型对同一问题多次生成回答的差异性

**目的**: 测试模型的创造性

**参数**: 温度=0.7 (较高温度增加多样性)

**计算方法**:
```
多样性与回答长度方差成正比
多样性得分 ∈ [0, 1]
0 = 所有回答完全相同
1 = 回答差异很大
```

**使用场景**:
- 需要多样答案的场景 (如头脑风暴)
- 避免模型过于机械

---

### 3. 一致性 (Consistency)

**定义**: 模型对同一问题多次生成回答的相似度

**目的**: 测试模型的稳定性

**参数**: 温度=0.1 (低温度保持一致性)

**计算方法**:
```
基于词汇重叠度计算相似度
一致性得分 ∈ [0, 1]
0 = 完全不一致
1 = 完全一致
```

**使用场景**:
- 需要稳定回答的场景 (如FAQ)
- 确保事实准确

---

### 4. 测试集评估 (Test Set Evaluation)

**定义**: 从训练数据中随机采样测试

**方法**:
1. 从测试数据中随机采样N条
2. 比较模型回答与预期答案
3. 计算相似度（基于词重叠）

**问题**:
⚠️ 当前评估结果0%不合格可能因为：
- 评估方法过于严格（词重叠相似度）
- 训练数据包含大量未见过的内容
- 模型生成的表达方式与训练数据不同

**改进方向**:
- 使用语义相似度（如embedding）
- 人工抽检部分样本
- 调整相似度阈值

---

## 🎯 提升模型效果的建议

### 当前问题分析

**1. 数字记忆不准确**
```
问题: 课题协作费多少金额需签合同？
预期: 10000元以上
实际: 10万元以上（错误）
```

**2. 办公用品问题列举过多**
```
问题: 办公用品报销规定？
实际: 列举了几十种办公用品，但没有说清楚规定
```

**3. 测试集相似度低**
```
可能原因:
- 模型生成方式与训练数据不同
- 评估指标过于严格
- 需要更好的语义相似度计算
```

---

### 改进方案

#### 方案1: 增加训练数据
```bash
# 提高数据蒸馏密度
修改 src/data_prep/distiller.py
num_pairs = 15  # 从10增加到15
```

#### 方案2: 调整训练参数
```yaml
# config.yaml
training:
  num_epochs: 5  # 从3增加到5
  learning_rate: 0.00003  # 降低学习率
lora:
  rank: 32  # 从16增加
  alpha: 64  # 从32增加
```

#### 方案3: 针对性优化
- 为数字类问题增加专门训练数据
- 强调金额阈值等重要信息
- 使用结构化输出格式

#### 方案4: 多轮训练
1. 第一轮: 整体训练
2. 第二轮: 针对薄弱环节（数字、细节）
3. 第三轮: 强化核心流程

---

## 🚀 快速评估流程

### 日常评估（5分钟）
```bash
# 快速测试几个关键问题
python3 test_trained_model.py
```

### 完整评估（30分钟）
```bash
# 1. 自动化批量评估
python3 evaluate_model.py

# 2. 对比基座模型
python3 compare_models.py

# 3. 查看评估报告
cat evaluation_report_*.json
```

### 交互式评估（按需）
```bash
# 手动探索性测试
python3 interactive_eval.py
```

---

## 📝 评估报告解读

### 综合评分计算
```python
综合评分 = 精确度 × 0.6 + 一致性 × 0.3 + 多样性 × 0.1

# 当前:
综合评分 = 0.80 × 0.6 + 0.50 × 0.3 + 0.20 × 0.1
        = 0.48 + 0.15 + 0.02
        = 0.65
```

### 评级标准
| 分数范围 | 评级 | 说明 |
|---------|------|------|
| 80-100 | ⭐⭐⭐⭐⭐ 优秀 | 可直接部署 |
| 70-79 | ⭐⭐⭐⭐ 良好 | 基本可用，小修小补 |
| 60-69 | ⭐⭐⭐ 合格 | 需要优化后使用 |
| <60 | ⭐⭐ 需改进 | 需要重新训练 |

---

## 🔧 高级评估技巧

### 1. 创建自定义测试集
```python
# 在 evaluate_model.py 中添加
custom_test_cases = [
    {
        "question": "你的问题",
        "keywords": ["关键词1", "关键词2"],
        "description": "测试描述"
    },
    # ...
]
```

### 2. A/B测试
```bash
# 训练两个不同配置的模型
# 使用 compare_models.py 对比
```

### 3. 错误分析
```bash
# 查看评估报告中失败案例
# 分析原因：
# - 数据问题？
# - 训练不足？
# - 模型能力限制？
```

### 4. 人工评估
```
邀请领域专家评估:
1. 准确性 - 回答是否正确
2. 完整性 - 是否遗漏重要信息
3. 可用性 - 是否能解决实际问题
4. 专业性 - 是否符合领域规范
```

---

## 📌 总结

### 当前状态
✅ **模型可用** - 65分，达到合格水平
✅ **核心流程掌握好** - 差旅费、会议费等回答准确
⚠️ **细节需优化** - 数字记忆、列举式回答

### 下一步
1. **短期**: 手动测试关键场景，确认可用性
2. **中期**: 针对薄弱环节增加训练数据
3. **长期**: 建立持续评估和优化机制

### 评估最佳实践
- 🔄 定期评估（每次训练后）
- 📊 量化 + 人工结合
- 📝 记录评估结果，追踪改进
- 🎯 关注核心指标，不要追求完美

---

**祝评估顺利！** 🎉
